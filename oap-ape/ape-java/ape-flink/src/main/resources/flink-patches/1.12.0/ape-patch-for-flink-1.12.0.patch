From 95a8215a29060230bdce8fd3c6c20c8dd428d0a3 Mon Sep 17 00:00:00 2001
From: iyupeng <peng.yu@intel.com>
Date: Wed, 19 May 2021 10:52:21 +0800
Subject: [PATCH] merge flink changes (#8)

* squashed internal dev changes
---
 .../hive/ExpressionToPredicateConverter.java  | 733 ++++++++++++++++++
 .../flink/connectors/hive/HiveOptions.java    |  38 +
 .../connectors/hive/HiveTableSource.java      | 272 ++++++-
 .../connectors/hive/PredicateOptimizers.java  | 213 +++++
 .../org/apache/flink/core/fs/FileSystem.java  |   1 +
 .../flink-cached-fs-hadoop/pom.xml            | 188 +++++
 .../pmemhadoop/CachedFileSystemFactory.java   | 101 +++
 .../src/main/resources/META-INF/NOTICE        |  21 +
 ...org.apache.flink.core.fs.FileSystemFactory |  16 +
 .../src/main/resources/licenses/LICENSE.jdom  |  51 ++
 .../fs/pmemhadoop/CachedFileSystemTest.java   |  66 ++
 .../src/test/resources/log4j2-test.properties |   8 +
 flink-filesystems/pom.xml                     |   1 +
 flink-formats/flink-parquet/pom.xml           |  16 +
 .../ParquetColumnarRowInputFormat.java        |   3 +-
 .../flink/formats/parquet/ape/ConfKeys.java   |  30 +
 ...ustomizedParquetVectorizedInputFormat.java | 602 ++++++++++++++
 .../SupportsAggregationPushDown.java          |  66 ++
 .../flink/table/utils/ape/AggregateExpr.java  | 140 ++++
 .../utils/ape/AggregateExprBinaryOper.java    |  96 +++
 .../table/utils/ape/AggregateExprColumn.java  |  42 +
 .../table/utils/ape/AggregateExprLiteral.java |  42 +
 .../table/utils/ape/AggregateExprRoot.java    |  61 ++
 .../utils/ape/AggregateExprRootChild.java     |  78 ++
 .../flink/table/utils/ape/AggregateExprs.java |  55 ++
 .../plan/rules/FlinkBatchRuleSets.scala       |   4 +-
 .../physical/batch/BatchExecAggRuleBase.scala |  67 +-
 ...alHashAggCalcIntoTableSourceScanRule.scala | 233 ++++++
 ...hLocalHashAggIntoTableSourceScanRule.scala | 160 ++++
 .../table/filesystem/LimitableBulkFormat.java |  10 +-
 30 files changed, 3401 insertions(+), 13 deletions(-)
 create mode 100644 flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ExpressionToPredicateConverter.java
 create mode 100644 flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/PredicateOptimizers.java
 create mode 100644 flink-filesystems/flink-cached-fs-hadoop/pom.xml
 create mode 100644 flink-filesystems/flink-cached-fs-hadoop/src/main/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemFactory.java
 create mode 100644 flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/NOTICE
 create mode 100644 flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/services/org.apache.flink.core.fs.FileSystemFactory
 create mode 100644 flink-filesystems/flink-cached-fs-hadoop/src/main/resources/licenses/LICENSE.jdom
 create mode 100644 flink-filesystems/flink-cached-fs-hadoop/src/test/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemTest.java
 create mode 100644 flink-filesystems/flink-cached-fs-hadoop/src/test/resources/log4j2-test.properties
 create mode 100644 flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/ConfKeys.java
 create mode 100644 flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/CustomizedParquetVectorizedInputFormat.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/source/abilities/SupportsAggregationPushDown.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExpr.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprBinaryOper.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprColumn.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprLiteral.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRoot.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRootChild.java
 create mode 100644 flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprs.java
 create mode 100644 flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggCalcIntoTableSourceScanRule.scala
 create mode 100644 flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggIntoTableSourceScanRule.scala

diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ExpressionToPredicateConverter.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ExpressionToPredicateConverter.java
new file mode 100644
index 0000000000..fab460ad71
--- /dev/null
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ExpressionToPredicateConverter.java
@@ -0,0 +1,733 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connectors.hive;
+
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.table.data.DecimalDataUtils;
+import org.apache.flink.table.expressions.CallExpression;
+import org.apache.flink.table.expressions.Expression;
+import org.apache.flink.table.expressions.ExpressionVisitor;
+import org.apache.flink.table.expressions.FieldReferenceExpression;
+import org.apache.flink.table.expressions.ResolvedExpression;
+import org.apache.flink.table.expressions.TypeLiteralExpression;
+import org.apache.flink.table.expressions.ValueLiteralExpression;
+import org.apache.flink.table.functions.BuiltInFunctionDefinition;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+
+import com.intel.ape.parquet.ApeContainsFilter;
+import com.intel.ape.parquet.ApeEndWithFilter;
+import com.intel.ape.parquet.ApeStartWithFilter;
+import org.apache.parquet.filter2.predicate.FilterApi;
+import org.apache.parquet.filter2.predicate.FilterPredicate;
+import org.apache.parquet.filter2.predicate.Operators;
+import org.apache.parquet.io.api.Binary;
+
+import javax.annotation.Nullable;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.time.LocalDate;
+import java.time.temporal.ChronoField;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.List;
+import java.util.TimeZone;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import static java.math.BigDecimal.ROUND_UNNECESSARY;
+import static java.util.Calendar.DAY_OF_MONTH;
+import static java.util.Calendar.ERA;
+import static java.util.Calendar.MONTH;
+import static java.util.Calendar.YEAR;
+
+/**
+ * Visit expression to generator {@link FilterPredicate}.
+ */
+public class ExpressionToPredicateConverter implements ExpressionVisitor<FilterPredicate> {
+	private static final String FUNC_NOT = "not";
+	private static final String FUNC_AND = "and";
+	private static final String FUNC_OR = "or";
+	private static final String FUNC_IS_NULL = "isNull";
+	private static final String FUNC_IS_NOT_NULL = "isNotNull";
+	private static final String FUNC_EQUALS = "equals";
+	private static final String FUNC_NOT_EQUALS = "notEquals";
+	private static final String FUNC_GREATER_THAN = "greaterThan";
+	private static final String FUNC_GREATER_THAN_OR_EQUAL = "greaterThanOrEqual";
+	private static final String FUNC_LESS_THAN = "lessThan";
+	private static final String FUNC_LESS_THAN_OR_EQUAL = "lessThanOrEqual";
+	private static final String FUNC_LIKE = "like";
+
+	private static final int MILLIS_PER_DAY = 86400 * 1000;
+
+	@Override
+	public FilterPredicate visit(CallExpression call) {
+		if (!(call.getFunctionDefinition() instanceof BuiltInFunctionDefinition)) {
+			return null;
+		}
+
+		String name = ((BuiltInFunctionDefinition) call.getFunctionDefinition()).getName();
+		switch (name) {
+			case FUNC_NOT:
+				FilterPredicate child = call.getChildren().get(0).accept(this);
+				if (child != null) {
+					return FilterApi.not(child);
+				}
+				break;
+			case FUNC_AND:
+				FilterPredicate ac1 = call.getChildren().get(0).accept(this);
+				FilterPredicate ac2 = call.getChildren().get(1).accept(this);
+				if (ac1 != null && ac2 != null) {
+					return FilterApi.and(ac1, ac2);
+				}
+				break;
+			case FUNC_OR:
+				FilterPredicate oc1 = call.getChildren().get(0).accept(this);
+				FilterPredicate oc2 = call.getChildren().get(1).accept(this);
+				if (oc1 != null && oc2 != null) {
+					return FilterApi.or(oc1, oc2);
+				}
+				break;
+			case FUNC_IS_NULL:
+			case FUNC_IS_NOT_NULL:
+				Operators.Column column = extractColumn(call.getResolvedChildren());
+
+				if (column == null) {
+					return null;
+				}
+
+				switch (name) {
+					case FUNC_IS_NULL:
+						return equals(new Tuple2<>(column, null));
+					case FUNC_IS_NOT_NULL:
+						return notEquals(new Tuple2<>(column, null));
+					default:
+						return null;
+				}
+
+			case FUNC_EQUALS:
+			case FUNC_NOT_EQUALS:
+			case FUNC_GREATER_THAN:
+			case FUNC_GREATER_THAN_OR_EQUAL:
+			case FUNC_LESS_THAN:
+			case FUNC_LESS_THAN_OR_EQUAL:
+			case FUNC_LIKE:
+				Tuple2<Operators.Column, Comparable> columnPair =
+					extractColumnAndLiteral(call.getResolvedChildren());
+
+				if (columnPair == null) {
+					return null;
+				}
+
+				boolean onRight = literalOnRight(call.getResolvedChildren());
+				switch (name) {
+					case FUNC_EQUALS:
+						return equals(columnPair);
+					case FUNC_NOT_EQUALS:
+						return notEquals(columnPair);
+					case FUNC_GREATER_THAN:
+						if (onRight) {
+							return greaterThan(columnPair);
+						} else {
+							lessThan(columnPair);
+						}
+						break;
+					case FUNC_GREATER_THAN_OR_EQUAL:
+						if (onRight) {
+							return greaterThanOrEqual(columnPair);
+						} else {
+							return lessThanOrEqual(columnPair);
+						}
+					case FUNC_LESS_THAN:
+						if (onRight) {
+							return lessThan(columnPair);
+						} else {
+							return greaterThan(columnPair);
+						}
+					case FUNC_LESS_THAN_OR_EQUAL:
+						if (onRight) {
+							return lessThanOrEqual(columnPair);
+						} else {
+							return greaterThanOrEqual(columnPair);
+						}
+					case FUNC_LIKE:
+						return like(columnPair, call.getResolvedChildren());
+					default:
+						// Unsupported Predicate
+						return null;
+				}
+		}
+
+		return null;
+	}
+
+	@Nullable
+	private FilterPredicate equals(Tuple2<Operators.Column, Comparable> columnPair) {
+		if (columnPair.f0 instanceof Operators.IntColumn) {
+			return FilterApi.eq((Operators.IntColumn) columnPair.f0, (Integer) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.LongColumn) {
+			return FilterApi.eq((Operators.LongColumn) columnPair.f0, (Long) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.DoubleColumn) {
+			return FilterApi.eq((Operators.DoubleColumn) columnPair.f0, (Double) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.FloatColumn) {
+			return FilterApi.eq((Operators.FloatColumn) columnPair.f0, (Float) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.BooleanColumn) {
+			return FilterApi.eq((Operators.BooleanColumn) columnPair.f0, (Boolean) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.BinaryColumn) {
+			return FilterApi.eq((Operators.BinaryColumn) columnPair.f0, (Binary) columnPair.f1);
+		}
+
+		return null;
+	}
+
+	@Nullable
+	private FilterPredicate notEquals(Tuple2<Operators.Column, Comparable> columnPair) {
+		if (columnPair.f0 instanceof Operators.IntColumn) {
+			return FilterApi.notEq((Operators.IntColumn) columnPair.f0, (Integer) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.LongColumn) {
+			return FilterApi.notEq((Operators.LongColumn) columnPair.f0, (Long) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.DoubleColumn) {
+			return FilterApi.notEq((Operators.DoubleColumn) columnPair.f0, (Double) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.FloatColumn) {
+			return FilterApi.notEq((Operators.FloatColumn) columnPair.f0, (Float) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.BooleanColumn) {
+			return FilterApi.notEq(
+				(Operators.BooleanColumn) columnPair.f0,
+				(Boolean) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.BinaryColumn) {
+			return FilterApi.notEq((Operators.BinaryColumn) columnPair.f0, (Binary) columnPair.f1);
+		}
+
+		return null;
+	}
+
+	@Nullable
+	private FilterPredicate greaterThan(Tuple2<Operators.Column, Comparable> columnPair) {
+		if (columnPair.f0 instanceof Operators.IntColumn) {
+			return FilterApi.gt((Operators.IntColumn) columnPair.f0, (Integer) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.LongColumn) {
+			return FilterApi.gt((Operators.LongColumn) columnPair.f0, (Long) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.DoubleColumn) {
+			return FilterApi.gt((Operators.DoubleColumn) columnPair.f0, (Double) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.FloatColumn) {
+			return FilterApi.gt((Operators.FloatColumn) columnPair.f0, (Float) columnPair.f1);
+		}
+
+		return null;
+	}
+
+	@Nullable
+	private FilterPredicate lessThan(Tuple2<Operators.Column, Comparable> columnPair) {
+		if (columnPair.f0 instanceof Operators.IntColumn) {
+			return FilterApi.lt((Operators.IntColumn) columnPair.f0, (Integer) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.LongColumn) {
+			return FilterApi.lt((Operators.LongColumn) columnPair.f0, (Long) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.DoubleColumn) {
+			return FilterApi.lt((Operators.DoubleColumn) columnPair.f0, (Double) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.FloatColumn) {
+			return FilterApi.lt((Operators.FloatColumn) columnPair.f0, (Float) columnPair.f1);
+		}
+
+		return null;
+	}
+
+	@Nullable
+	private FilterPredicate greaterThanOrEqual(Tuple2<Operators.Column, Comparable> columnPair) {
+		if (columnPair.f0 instanceof Operators.IntColumn) {
+			return FilterApi.gtEq((Operators.IntColumn) columnPair.f0, (Integer) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.LongColumn) {
+			return FilterApi.gtEq((Operators.LongColumn) columnPair.f0, (Long) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.DoubleColumn) {
+			return FilterApi.gtEq((Operators.DoubleColumn) columnPair.f0, (Double) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.FloatColumn) {
+			return FilterApi.gtEq((Operators.FloatColumn) columnPair.f0, (Float) columnPair.f1);
+		}
+
+		return null;
+	}
+
+	@Nullable
+	private FilterPredicate lessThanOrEqual(Tuple2<Operators.Column, Comparable> columnPair) {
+		if (columnPair.f0 instanceof Operators.IntColumn) {
+			return FilterApi.ltEq((Operators.IntColumn) columnPair.f0, (Integer) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.LongColumn) {
+			return FilterApi.ltEq((Operators.LongColumn) columnPair.f0, (Long) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.DoubleColumn) {
+			return FilterApi.ltEq((Operators.DoubleColumn) columnPair.f0, (Double) columnPair.f1);
+		} else if (columnPair.f0 instanceof Operators.FloatColumn) {
+			return FilterApi.ltEq((Operators.FloatColumn) columnPair.f0, (Float) columnPair.f1);
+		}
+
+		return null;
+	}
+
+	private Operators.Column extractColumn(List<ResolvedExpression> children) {
+
+		if (children == null || children.size() != 1) {
+			return null;
+		}
+
+		boolean isValid = children.get(0) instanceof FieldReferenceExpression;
+
+		if (!isValid) {
+			return null;
+		}
+
+		String columnName = ((FieldReferenceExpression) children.get(0)).getName();
+		DataType columnType = children.get(0).getOutputDataType();
+
+		return makeColumn(columnName, columnType);
+
+	}
+
+	private Tuple2<Operators.Column, Comparable> extractColumnAndLiteral(
+		List<ResolvedExpression> children) {
+
+		// check column type and value type match
+		boolean isValid = checkColumnAndLiteralTypes(children);
+
+		if (!isValid) {
+			return null;
+		}
+
+		Operators.Column column = getColumn(children);
+		Comparable value = getValue(children);
+
+		if (column != null
+			&& value != null
+			&& isDecimalColumn(children)) {
+
+			int columnScale = getDecimalColumnScale(children);
+			value = getDecimalValueMatchingColumnScale(column, columnScale, value);
+		}
+
+		if (column == null || value == null) {
+			return null;
+		}
+
+		return new Tuple2<>(column, value);
+
+	}
+
+	/**
+	 * set value scale to match column type's scale when it's Decimal.
+	 * @param column
+	 * @param columnScale
+	 * @param literal
+	 * @return
+	 */
+	private Comparable getDecimalValueMatchingColumnScale(
+		Operators.Column column, int columnScale, Comparable literal) {
+
+		BigDecimal literalDecimal = new BigDecimal(literal.toString());
+		if (literalDecimal.scale() > columnScale) {
+			// not supported
+			return null;
+		} else if (literalDecimal.scale() < columnScale) {
+			// promote the scale of literal value
+			literalDecimal = literalDecimal.setScale(columnScale, ROUND_UNNECESSARY);
+		}
+
+		Comparable value = null;
+		BigInteger literalInteger = literalDecimal.unscaledValue();
+
+		// the literal value's type shouldn't be larger than the type of column
+		if (column instanceof Operators.IntColumn
+			&& DecimalDataUtils.is32BitDecimal(literalDecimal.precision())) {
+			value = literalInteger.intValue();
+		} else if (
+			column instanceof Operators.LongColumn
+			&& (DecimalDataUtils.is32BitDecimal(literalDecimal.precision())
+				|| DecimalDataUtils.is64BitDecimal(literalDecimal.precision())
+			)
+		) {
+			value = literalInteger.longValue();
+		}
+
+		return value;
+	}
+
+	private boolean isDecimalColumn(List<ResolvedExpression> children) {
+		int columnIndex = 0;
+		if (!literalOnRight(children)) {
+			columnIndex = 1;
+		}
+
+		DataType columnType = children.get(columnIndex).getOutputDataType();
+
+		return columnType.getLogicalType().getTypeRoot() == LogicalTypeRoot.DECIMAL;
+	}
+
+	private int getDecimalColumnScale(List<ResolvedExpression> children) {
+		int columnIndex = 0;
+		if (!literalOnRight(children)) {
+			columnIndex = 1;
+		}
+
+		DataType columnType = children.get(columnIndex).getOutputDataType();
+
+		if (columnType.getLogicalType() instanceof DecimalType) {
+			return ((DecimalType) columnType.getLogicalType()).getScale();
+		} else {
+			throw new RuntimeException("Invalid call to getDecimalColumnScale");
+		}
+	}
+
+	private boolean checkColumnAndLiteralTypes(List<ResolvedExpression> children) {
+		if (children == null || children.size() != 2) {
+			return false;
+		}
+
+		boolean isValid = (
+			(children.get(0) instanceof FieldReferenceExpression
+				&& children.get(1) instanceof ValueLiteralExpression)
+				||
+				(children.get(1) instanceof FieldReferenceExpression
+					&& children.get(0) instanceof ValueLiteralExpression));
+
+		if (!isValid) {
+			return false;
+		}
+
+		int columnIndex = 0;
+		int valueIndex = 1;
+		if (!literalOnRight(children)) {
+			columnIndex = 1;
+			valueIndex = 0;
+		}
+
+		LogicalTypeRoot columnType =
+			children.get(columnIndex).getOutputDataType().getLogicalType().getTypeRoot();
+		LogicalTypeRoot valueType =
+			children.get(valueIndex).getOutputDataType().getLogicalType().getTypeRoot();
+
+		// column type and value type should match
+		switch (columnType) {
+			case BOOLEAN:
+				return valueType == LogicalTypeRoot.BOOLEAN;
+			case TINYINT:
+			case SMALLINT:
+			case INTEGER:
+				// integer numbers are all INTEGER in ValueLiteralExpressions
+				return valueType == LogicalTypeRoot.INTEGER;
+			case DATE:
+				return valueType == LogicalTypeRoot.DATE;
+			case BIGINT:
+				return valueType == LogicalTypeRoot.BIGINT || valueType == LogicalTypeRoot.INTEGER;
+			case FLOAT:
+				return valueType == LogicalTypeRoot.FLOAT || valueType == LogicalTypeRoot.INTEGER;
+			case DOUBLE:
+				return valueType == LogicalTypeRoot.DOUBLE
+					|| valueType == LogicalTypeRoot.FLOAT
+					|| valueType == LogicalTypeRoot.BIGINT
+					|| valueType == LogicalTypeRoot.INTEGER;
+			case DECIMAL:
+			case BINARY:
+			case VARBINARY:
+			case CHAR:
+			case VARCHAR:
+				// No data loss between values and strings in these types
+				return true;
+			default:
+				// unsupported types
+				return false;
+		}
+
+	}
+
+	private boolean literalOnRight(List<ResolvedExpression> children) {
+		return children.get(1) instanceof ValueLiteralExpression;
+	}
+
+	@Nullable
+	private Operators.Column getColumn(List<ResolvedExpression> children) {
+		int columnIndex = 0;
+		if (!literalOnRight(children)) {
+			columnIndex = 1;
+		}
+
+		String columnName = ((FieldReferenceExpression) children.get(columnIndex)).getName();
+		DataType columnType = children.get(columnIndex).getOutputDataType();
+
+		return makeColumn(columnName, columnType);
+
+	}
+
+	@Nullable
+	private Operators.Column makeColumn(String columnName, DataType columnType) {
+
+		switch (columnType.getLogicalType().getTypeRoot()) {
+			case TINYINT:
+			case SMALLINT:
+			case DATE:
+			case INTEGER:
+				return FilterApi.intColumn(columnName);
+			case BIGINT:
+				return FilterApi.longColumn(columnName);
+			case FLOAT:
+				return FilterApi.floatColumn(columnName);
+			case DOUBLE:
+				return FilterApi.doubleColumn(columnName);
+			case DECIMAL:
+				DecimalType decimalType = (DecimalType) columnType.getLogicalType();
+				if (DecimalDataUtils.is32BitDecimal(decimalType.getPrecision())) {
+					return FilterApi.intColumn(columnName);
+				} else if (DecimalDataUtils.is64BitDecimal(decimalType.getPrecision())) {
+					return FilterApi.longColumn(columnName);
+				} else {
+					return null;
+				}
+			case BINARY:
+			case VARBINARY:
+			case CHAR:
+			case VARCHAR:
+				return FilterApi.binaryColumn(columnName);
+			case BOOLEAN:
+				return FilterApi.booleanColumn(columnName);
+			default:
+				// unsupported types
+				return null;
+		}
+
+	}
+
+	@Nullable
+	private Comparable getValue(List<ResolvedExpression> children) {
+		int valueIndex = 1;
+		if (!literalOnRight(children)) {
+			valueIndex = 0;
+		}
+
+		ValueLiteralExpression ex = (ValueLiteralExpression) children.get(valueIndex);
+		DataType valueType = ex.getOutputDataType();
+
+		switch (valueType.getLogicalType().getTypeRoot()) {
+			case TINYINT:
+			case SMALLINT:
+			case INTEGER:
+				return ex.getValueAs(Integer.class).orElse(null);
+			case DATE:
+				Object obj = ex.getValueAs(Object.class).orElse(null);
+				return dateToDays(obj);
+			case BIGINT:
+				return ex.getValueAs(Long.class).orElse(null);
+			case FLOAT:
+				return ex.getValueAs(Float.class).orElse(null);
+			case DOUBLE:
+				return ex.getValueAs(Double.class).orElse(null);
+			case DECIMAL:
+				return ex.getValueAs(BigDecimal.class).orElse(null);
+			case BINARY:
+			case VARBINARY:
+				byte[] bytes = ex.getValueAs(byte[].class).orElse(null);
+				return bytes == null ? null : Binary.fromConstantByteArray(bytes);
+			case CHAR:
+			case VARCHAR:
+				String s = ex.getValueAs(String.class).orElse(null);
+				return s == null ? null : Binary.fromString(s);
+			case BOOLEAN:
+				return ex.getValueAs(Boolean.class).orElse(null);
+			default:
+				// unsupported types
+				return null;
+		}
+	}
+
+	private Integer dateToDays(Object obj) {
+		if (obj instanceof Date) {
+			Date date = (Date) obj;
+			long millisUtc = date.getTime();
+			long millisLocal = millisUtc + TimeZone.getDefault().getOffset(millisUtc);
+			int julianDays = Math.toIntExact(Math.floorDiv(millisLocal, MILLIS_PER_DAY));
+
+			Calendar utcCal = new Calendar.Builder()
+				// `gregory` is a hybrid calendar that supports both
+				// the Julian and Gregorian calendar systems
+				.setCalendarType("gregory")
+				.setTimeZone(TimeZone.getTimeZone("UTC"))
+				.setInstant(Math.multiplyExact(julianDays, MILLIS_PER_DAY))
+				.build();
+			LocalDate localDate = LocalDate.of(
+				utcCal.get(YEAR),
+				utcCal.get(MONTH) + 1,
+				// The number of days will be added later to handle non-existing
+				// Julian dates in Proleptic Gregorian calendar.
+				// For example, 1000-02-29 exists in Julian calendar because 1000
+				// is a leap year but it is not a leap year in Gregorian calendar.
+				1)
+				.with(ChronoField.ERA, utcCal.get(ERA))
+				.plusDays(utcCal.get(DAY_OF_MONTH) - 1);
+
+			return Math.toIntExact(localDate.toEpochDay());
+		} else if (obj instanceof LocalDate) {
+			return Math.toIntExact(((LocalDate) obj).toEpochDay());
+		}
+
+		return null;
+	}
+
+	/**
+	 * Try to parse `like` to `equals`, `startsWith`, `endsWith`, `contains` if applicable.
+	 * @param columnPair Tuple2
+	 * @param children List
+	 * @return FilterPredicate or null
+	 */
+	@Nullable
+	private FilterPredicate like(
+		Tuple2<Operators.Column, Comparable> columnPair,
+		List<ResolvedExpression> children) {
+
+		// check data types
+		if (!(columnPair.f0 instanceof Operators.BinaryColumn)
+			|| !(columnPair.f1 instanceof Binary)) {
+			return null;
+		}
+
+		// get expression value as String
+		String likeValue = getStringValue(children);
+		if (likeValue == null) {
+			return null;
+		}
+
+		// return predicate
+		String slicedValue;
+		if (isPlainLikeValue(likeValue)) {
+			return FilterApi.eq((Operators.BinaryColumn) columnPair.f0, (Binary) columnPair.f1);
+		} else if ((slicedValue = getStartsWithFromLikeValue(likeValue)) != null) {
+			return FilterApi.userDefined(columnPair.f0, new ApeStartWithFilter(slicedValue));
+		} else if ((slicedValue = getEndsWithFromLikeValue(likeValue)) != null) {
+			return FilterApi.userDefined(columnPair.f0, new ApeEndWithFilter(slicedValue));
+		} else if ((slicedValue = getContainsWithFromLikeValue(likeValue)) != null) {
+			return FilterApi.userDefined(columnPair.f0, new ApeContainsFilter(slicedValue));
+		}
+
+		return null;
+	}
+
+	@Nullable
+	private String getStringValue(List<ResolvedExpression> children) {
+		// check data type of expression value
+		int valueIndex = 1;
+		if (!literalOnRight(children)) {
+			valueIndex = 0;
+		}
+		ValueLiteralExpression ex = (ValueLiteralExpression) children.get(valueIndex);
+		DataType valueType = ex.getOutputDataType();
+
+		// expression value must be able to cast to String
+		if (valueType.getLogicalType().getTypeRoot() != LogicalTypeRoot.CHAR
+			&& valueType.getLogicalType().getTypeRoot() != LogicalTypeRoot.VARCHAR) {
+			return null;
+		}
+
+		// get expression value as String
+		return ex.getValueAs(String.class).orElse(null);
+	}
+
+	/**
+	 * Check whether a `like` pattern contains special characters.
+	 *
+	 * @param likeValue String
+	 * @return boolean true if there is no special character. For example, "abc19 1x"
+	 */
+	private boolean isPlainLikeValue(String likeValue) {
+		Pattern pattern = Pattern.compile("^[^%_!^\\[\\]\\\\]*$");
+		return pattern.matcher(likeValue).matches();
+	}
+
+	/**
+	 * Try to get a `startsWith` value from a `like` pattern.
+	 * For example, "123%_" => "123". "123%abc%" => null.
+	 *
+	 * @param likeValue String
+	 * @return String the captured value contains no wildcards, or null if the pattern does not
+	 * just mean `startsWith`.
+	 */
+	@Nullable
+	private String getStartsWithFromLikeValue(String likeValue) {
+		Pattern pattern = Pattern.compile("^([^%_!^\\[\\]\\\\]+)[%_]+$");
+		Matcher matcher = pattern.matcher(likeValue);
+		if (matcher.matches()) {
+			return matcher.group(1);
+		}
+
+		return null;
+	}
+
+	/**
+	 * Try to get a `endsWith` value from a `like` pattern.
+	 * For example, "%123" => "123". "%123_abc" => null.
+	 *
+	 * @param likeValue String
+	 * @return String the captured value contains no wildcards, or null if the pattern does not
+	 * just mean `endsWith`.
+	 */
+	@Nullable
+	private String getEndsWithFromLikeValue(String likeValue) {
+		Pattern pattern = Pattern.compile("^[%_]+([^%_!^\\[\\]\\\\]+)$");
+		Matcher matcher = pattern.matcher(likeValue);
+		if (matcher.matches()) {
+			return matcher.group(1);
+		}
+
+		return null;
+	}
+
+	/**
+	 * Try to get a `contains` value from a `like` pattern.
+	 * For example, "%123%%" => "123". "%123%abc%" => null.
+	 *
+	 * @param likeValue String
+	 * @return String the captured value contains no wildcards, or null if the pattern does not
+	 * 	 * mean it `contains` extract one string.
+	 */
+	@Nullable
+	private String getContainsWithFromLikeValue(String likeValue) {
+		Pattern pattern = Pattern.compile("^[%_]+([^%_!^\\[\\]\\\\]+)[%_]+$");
+		Matcher matcher = pattern.matcher(likeValue);
+		if (matcher.matches()) {
+			return matcher.group(1);
+		}
+
+		return null;
+	}
+
+	@Override
+	public FilterPredicate visit(ValueLiteralExpression valueLiteral) {
+
+		return null;
+	}
+
+	@Override
+	public FilterPredicate visit(FieldReferenceExpression fieldReference) {
+		return null;
+	}
+
+	@Override
+	public FilterPredicate visit(TypeLiteralExpression typeLiteral) {
+		return null;
+	}
+
+	@Override
+	public FilterPredicate visit(Expression other) {
+		return null;
+	}
+}
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java
index 161bdaa494..86b7a4e5df 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java
@@ -52,4 +52,42 @@ public class HiveOptions {
 					.defaultValue(true)
 					.withDescription("If it is false, using flink native writer to write parquet and orc files; " +
 							"If it is true, using hadoop mapred record writer to write parquet and orc files.");
+
+	public static final ConfigOption<Boolean> TABLE_EXEC_HIVE_PARQUET_USE_NATIVE_READER =
+		key("table.exec.hive.parquet-native-reader")
+			.booleanType()
+			.defaultValue(false)
+			.withDescription("Option whether parquet input format uses native file reader. " +
+				"Native source reader may provide higher performance and support pushing down");
+
+	public static final ConfigOption<String> TABLE_EXEC_HIVE_PARQUET_NATIVE_READER_MODE =
+			key("table.exec.hive.parquet-native-reader.mode")
+					.stringType()
+					.defaultValue("local")
+					.withDescription("Mode of native file reader. Supported modes: local, remote.");
+
+	public static final ConfigOption<Boolean> TABLE_EXEC_HIVE_PARQUET_PUSH_DOWN_FILTERS =
+		key("table.exec.hive.parquet-push-down-filters")
+			.booleanType()
+			.defaultValue(false)
+			.withDescription("If it is true, enabling filters pushing down to source reader." +
+				"To enable pushing down, 'table.exec.hive.parquet-native-reader' is also needed to be set as true.");
+
+	public static final ConfigOption<Boolean> TABLE_EXEC_HIVE_PARQUET_FILTERS_EVADING_JOIN_REORDER =
+			key("table.exec.hive.parquet-filters-evading-join-reorder")
+					.booleanType()
+					.defaultValue(false)
+					.withDescription("If it is true, Flink will not push down filters when " +
+							"OptimizerConfigOptions.TABLE_OPTIMIZER_JOIN_REORDER_ENABLED is true." +
+							"Join-reorder may make worse performance when table scan get lower priority " +
+							"than before in joins after filters are pushed down.");
+
+	public static final ConfigOption<Boolean> TABLE_EXEC_HIVE_PARQUET_PUSH_DOWN_AGGREGATIONS =
+		key("table.exec.hive.parquet-push-down-aggregations")
+			.booleanType()
+			.defaultValue(false)
+			.withDescription("If it is true, enabling aggregations pushing down to source reader." +
+				"To enable pushing down, 'table.exec.hive.parquet-native-reader' and " +
+				"'table.exec.hive.parquet-push-down-filters' are also needed to be set as true.");
+
 }
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java
index 6b87c90a7b..f732ad92bb 100644
--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java
@@ -32,6 +32,7 @@ import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSource;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.config.OptimizerConfigOptions;
 import org.apache.flink.table.catalog.CatalogTable;
 import org.apache.flink.table.catalog.ObjectPath;
 import org.apache.flink.table.catalog.hive.client.HiveShim;
@@ -41,27 +42,38 @@ import org.apache.flink.table.connector.ChangelogMode;
 import org.apache.flink.table.connector.source.DataStreamScanProvider;
 import org.apache.flink.table.connector.source.DynamicTableSource;
 import org.apache.flink.table.connector.source.ScanTableSource;
+import org.apache.flink.table.connector.source.abilities.SupportsAggregationPushDown;
+import org.apache.flink.table.connector.source.abilities.SupportsFilterPushDown;
 import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;
 import org.apache.flink.table.connector.source.abilities.SupportsPartitionPushDown;
 import org.apache.flink.table.connector.source.abilities.SupportsProjectionPushDown;
 import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.expressions.ResolvedExpression;
 import org.apache.flink.table.filesystem.ContinuousPartitionFetcher;
 import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.utils.ape.AggregateExprs;
 import org.apache.flink.util.Preconditions;
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.mapred.JobConf;
+import org.apache.parquet.filter2.predicate.FilterApi;
+import org.apache.parquet.filter2.predicate.FilterPredicate;
+import org.apache.parquet.hadoop.util.SerializationUtil;
 import org.apache.thrift.TException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import javax.annotation.Nullable;
 
+import java.io.IOException;
 import java.time.Duration;
+import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
@@ -82,7 +94,9 @@ public class HiveTableSource implements
 		ScanTableSource,
 		SupportsPartitionPushDown,
 		SupportsProjectionPushDown,
-		SupportsLimitPushDown {
+		SupportsLimitPushDown,
+		SupportsFilterPushDown,
+		SupportsAggregationPushDown {
 
 	private static final Logger LOG = LoggerFactory.getLogger(HiveTableSource.class);
 	private static final Duration DEFAULT_SCAN_MONITOR_INTERVAL = Duration.ofMinutes(1L);
@@ -100,6 +114,20 @@ public class HiveTableSource implements
 	protected int[] projectedFields;
 	private Long limit = null;
 
+	public static final String JOB_CONF_KEY_PREDICATES =
+		"parquet.private.native.reader.filter.predicate";
+	public static final String JOB_CONF_KEY_PREDICATES_HUMAN =
+		"parquet.private.native.reader.filter.predicate.human.readable";
+
+	public static final String JOB_CONF_KEY_AGGREGATES =
+		"parquet.private.native.reader.agg.expressions";
+
+	protected FilterPredicate predicates = null;
+
+	protected List<String> aggOutputNames;
+	protected List<DataType> aggOutputTypes;
+	protected AggregateExprs aggregateExprs;
+
 	public HiveTableSource(
 			JobConf jobConf, ReadableConfig flinkConf, ObjectPath tablePath, CatalogTable catalogTable) {
 		this.jobConf = Preconditions.checkNotNull(jobConf);
@@ -109,6 +137,16 @@ public class HiveTableSource implements
 		this.hiveVersion = Preconditions.checkNotNull(jobConf.get(HiveCatalogValidator.CATALOG_HIVE_VERSION),
 				"Hive version is not defined");
 		this.hiveShim = HiveShimLoader.loadHiveShim(hiveVersion);
+
+		jobConf.setBoolean(
+			HiveOptions.TABLE_EXEC_HIVE_PARQUET_USE_NATIVE_READER.key(),
+			flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_PARQUET_USE_NATIVE_READER)
+		);
+
+		jobConf.set(
+				HiveOptions.TABLE_EXEC_HIVE_PARQUET_NATIVE_READER_MODE.key(),
+				flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_PARQUET_NATIVE_READER_MODE)
+		);
 	}
 
 	@Override
@@ -130,6 +168,19 @@ public class HiveTableSource implements
 	protected DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {
 		validateScanConfigurations();
 		checkAcidTable(catalogTable, tablePath);
+
+		// reset filters in job conf
+		setFilterPredicate(predicates);
+		if (predicates != null) {
+			LOG.info("Filters are pushed down to parquet reader.");
+		}
+
+		// reset aggregates in job conf
+		setAggregateExpr(aggregateExprs);
+		if (aggregateExprs != null) {
+			LOG.info("Aggregates are pushed down to parquet reader.");
+		}
+
 		List<HiveTablePartition> allHivePartitions = getAllPartitions(
 				jobConf,
 				hiveVersion,
@@ -224,14 +275,19 @@ public class HiveTableSource implements
 
 	protected TableSchema getProducedTableSchema() {
 		TableSchema fullSchema = getTableSchema();
-		if (projectedFields == null) {
-			return fullSchema;
-		} else {
+		if (aggOutputNames != null) {
+			return TableSchema.builder().fields(
+				aggOutputNames.toArray(new String[0]),
+				aggOutputTypes.toArray(new DataType[0])
+			).build();
+		} else if (projectedFields != null) {
 			String[] fullNames = fullSchema.getFieldNames();
 			DataType[] fullTypes = fullSchema.getFieldDataTypes();
 			return TableSchema.builder().fields(
-					Arrays.stream(projectedFields).mapToObj(i -> fullNames[i]).toArray(String[]::new),
-					Arrays.stream(projectedFields).mapToObj(i -> fullTypes[i]).toArray(DataType[]::new)).build();
+				Arrays.stream(projectedFields).mapToObj(i -> fullNames[i]).toArray(String[]::new),
+				Arrays.stream(projectedFields).mapToObj(i -> fullTypes[i]).toArray(DataType[]::new)).build();
+		} else {
+			return fullSchema;
 		}
 	}
 
@@ -281,9 +337,213 @@ public class HiveTableSource implements
 		source.remainingPartitions = remainingPartitions;
 		source.projectedFields = projectedFields;
 		source.limit = limit;
+
+		source.predicates = predicates;
+		source.aggOutputNames = aggOutputNames;
+		source.aggOutputTypes = aggOutputTypes;
+		source.aggregateExprs = aggregateExprs;
 		return source;
 	}
 
+	@Override
+	public Result applyFilters(List<ResolvedExpression> filters) {
+		// check if APE is applicable
+		if (!isAPENativeFilterApplicable()) {
+			return Result.of(Collections.emptyList(), filters);
+		}
+
+		// convert filters from Expressions to FilterPredicate
+		List<FilterPredicate> convertedPredicates = new ArrayList<>(filters.size());
+		List<ResolvedExpression> acceptedFilters = new ArrayList<>(filters.size());
+		List<ResolvedExpression> remainingFilters = new ArrayList<>(filters.size());
+
+		for (ResolvedExpression filter : filters) {
+			FilterPredicate convertedPredicate = filter.accept(new ExpressionToPredicateConverter());
+			if (convertedPredicate != null) {
+				convertedPredicates.add(convertedPredicate);
+				acceptedFilters.add(filter);
+			} else {
+				remainingFilters.add(filter);
+			}
+		}
+
+		// construct single Parquet FilterPredicate
+		FilterPredicate parquetPredicate = null;
+		if (!convertedPredicates.isEmpty()) {
+			// concat converted predicates with AND
+			parquetPredicate = convertedPredicates.get(0);
+
+			for (FilterPredicate converted : convertedPredicates.subList(1, convertedPredicates.size())) {
+				parquetPredicate = FilterApi.and(parquetPredicate, converted);
+			}
+
+			// optimize the filter tree
+			parquetPredicate = parquetPredicate.accept(new PredicateOptimizers.AddNotNull());
+		}
+
+		// Here, just set predicate to table source.
+		// We don't set predicate to jobConf now to avoid overriding filters.
+		predicates = parquetPredicate;
+
+		// remove pushed filters from remainingFilters
+		return Result.of(acceptedFilters, remainingFilters);
+	}
+
+	private void setFilterPredicate(FilterPredicate parquetPredicate) {
+		if (parquetPredicate == null) {
+			jobConf.unset(JOB_CONF_KEY_PREDICATES_HUMAN);
+			jobConf.unset(JOB_CONF_KEY_PREDICATES);
+			return;
+		}
+
+		jobConf.set(JOB_CONF_KEY_PREDICATES_HUMAN, parquetPredicate.toString());
+
+		try {
+			SerializationUtil.writeObjectToConfAsBase64(JOB_CONF_KEY_PREDICATES,
+				parquetPredicate, jobConf);
+		} catch (IOException ex) {
+			throw new RuntimeException(ex);
+		}
+	}
+
+	private void setAggregateExpr(AggregateExprs aggregateExprs) {
+		if (aggregateExprs == null) {
+			jobConf.unset(JOB_CONF_KEY_AGGREGATES);
+			return;
+		}
+
+		try {
+			SerializationUtil.writeObjectToConfAsBase64(
+					JOB_CONF_KEY_AGGREGATES,
+					aggregateExprs,
+					jobConf);
+		} catch (IOException ex) {
+			throw new RuntimeException(ex);
+		}
+	}
+
+	private boolean allPartitionsAreParquet() {
+		List<HiveTablePartition> allHivePartitions = getAllPartitions(
+			jobConf,
+			hiveVersion,
+			tablePath,
+			catalogTable,
+			hiveShim,
+			remainingPartitions);
+
+		for (HiveTablePartition partition : allHivePartitions) {
+			boolean isParquet = partition.getStorageDescriptor().getSerdeInfo().getSerializationLib()
+				.toLowerCase().contains("parquet");
+
+			if (!isParquet) {
+				return false;
+			}
+		}
+
+		return true;
+	}
+
+	private boolean allColumnsSupportVectorization() {
+		RowType producedRowType = (RowType) getProducedTableSchema().toRowDataType().bridgedTo(RowData.class).getLogicalType();
+		for (RowType.RowField field : producedRowType.getFields()) {
+			if (isVectorizationUnsupported(field.getType())) {
+				LOG.info("Fallback to hadoop mapred reader, unsupported field type: " + field.getType());
+				return false;
+			}
+		}
+
+		return true;
+	}
+
+	private static boolean isVectorizationUnsupported(LogicalType t) {
+		switch (t.getTypeRoot()) {
+			case CHAR:
+			case VARCHAR:
+			case BOOLEAN:
+			case BINARY:
+			case VARBINARY:
+			case DECIMAL:
+			case TINYINT:
+			case SMALLINT:
+			case INTEGER:
+			case BIGINT:
+			case FLOAT:
+			case DOUBLE:
+			case DATE:
+			case TIME_WITHOUT_TIME_ZONE:
+			case TIMESTAMP_WITHOUT_TIME_ZONE:
+			case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
+				return false;
+			case TIMESTAMP_WITH_TIME_ZONE:
+			case INTERVAL_YEAR_MONTH:
+			case INTERVAL_DAY_TIME:
+			case ARRAY:
+			case MULTISET:
+			case MAP:
+			case ROW:
+			case DISTINCT_TYPE:
+			case STRUCTURED_TYPE:
+			case NULL:
+			case RAW:
+			case SYMBOL:
+			default:
+				return true;
+		}
+	}
+
+	@Override
+	public boolean applyAggregations(
+		List<String> outputNames,
+		List<DataType> outputTypes,
+		AggregateExprs aggregateExprs) {
+
+		if (!isAPENativeAggApplicable()) {
+			return false;
+		}
+
+		// transform data type names in aggregate expressions
+		try {
+			aggregateExprs.translateDataTypeName();
+		} catch (RuntimeException ex) {
+			LOG.warn(ex.toString());
+			return false;
+		}
+
+		aggOutputNames = outputNames;
+		aggOutputTypes = outputTypes;
+		this.aggregateExprs = aggregateExprs;
+
+		return true;
+	}
+
+	private boolean isAPENativeFilterApplicable() {
+
+		boolean useMapRedReader = flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER);
+		boolean useNativeParquetReader = flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_PARQUET_USE_NATIVE_READER);
+		boolean filterPushingDownEnabled = flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_PARQUET_PUSH_DOWN_FILTERS);
+		// join re-order does not know filters are pushed down.
+		// This may lead to worse efficiency when it moves backward tables with filters pushed down in joins.
+		boolean joinReorderEnabled = flinkConf.get(OptimizerConfigOptions.TABLE_OPTIMIZER_JOIN_REORDER_ENABLED);
+		// configuration that tells Flink whether push down filters or not when join re-order is enabled.
+		boolean evadingJoinReorder = flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_PARQUET_FILTERS_EVADING_JOIN_REORDER);
+
+		// check configurations
+		if (useMapRedReader
+				|| !useNativeParquetReader
+				|| !filterPushingDownEnabled
+				|| (joinReorderEnabled && evadingJoinReorder)) {
+			return false;
+		}
+
+		// check parquet format and column types
+		return allColumnsSupportVectorization() && allPartitionsAreParquet();
+	}
+
+	private boolean isAPENativeAggApplicable() {
+		boolean aggPushingDownEnabled = flinkConf.get(HiveOptions.TABLE_EXEC_HIVE_PARQUET_PUSH_DOWN_AGGREGATIONS);
+		return isAPENativeFilterApplicable() && aggPushingDownEnabled;
+	}
+
 	/**
 	 * PartitionFetcher.Context for {@link ContinuousPartitionFetcher}.
 	 */
diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/PredicateOptimizers.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/PredicateOptimizers.java
new file mode 100644
index 0000000000..8508a56dd1
--- /dev/null
+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/PredicateOptimizers.java
@@ -0,0 +1,213 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connectors.hive;
+
+import org.apache.flink.api.java.tuple.Tuple2;
+
+import org.apache.parquet.filter2.predicate.FilterApi;
+import org.apache.parquet.filter2.predicate.FilterPredicate;
+import org.apache.parquet.filter2.predicate.Operators;
+import org.apache.parquet.filter2.predicate.UserDefinedPredicate;
+import org.apache.parquet.hadoop.metadata.ColumnPath;
+import org.apache.parquet.io.api.Binary;
+
+import javax.annotation.Nullable;
+
+/**
+ * Optimizers to improve predicate tree.
+ */
+public class PredicateOptimizers {
+
+	/**
+	 * Add an `and` `is not null` brother to existing column predicate.
+	 */
+	static class AddNotNull implements FilterPredicate.Visitor<FilterPredicate> {
+		private FilterPredicate parent = null;
+
+		@Override
+		public <T extends Comparable<T>> FilterPredicate visit(Operators.Eq<T> eq) {
+			// `is null` does not need to add `not null` predicate
+			if (eq.getValue() == null) {
+				return eq;
+			}
+
+			if (needToBeNotNull(eq, eq.getColumn().getColumnPath())) {
+				return andNotNull(eq, eq.getColumn());
+			}
+
+			return eq;
+		}
+
+		@Override
+		public <T extends Comparable<T>> FilterPredicate visit(Operators.NotEq<T> notEq) {
+			// `is not null` does not need to add `not null` predicate
+			if (notEq.getValue() == null) {
+				return notEq;
+			}
+
+			if (needToBeNotNull(notEq, notEq.getColumn().getColumnPath())) {
+				return andNotNull(notEq, notEq.getColumn());
+			}
+
+			return notEq;
+		}
+
+		@Override
+		public <T extends Comparable<T>> FilterPredicate visit(Operators.Lt<T> lt) {
+			if (needToBeNotNull(lt, lt.getColumn().getColumnPath())) {
+				return andNotNull(lt, lt.getColumn());
+			}
+
+			return lt;
+		}
+
+		@Override
+		public <T extends Comparable<T>> FilterPredicate visit(Operators.LtEq<T> ltEq) {
+			if (needToBeNotNull(ltEq, ltEq.getColumn().getColumnPath())) {
+				return andNotNull(ltEq, ltEq.getColumn());
+			}
+
+			return ltEq;
+		}
+
+		@Override
+		public <T extends Comparable<T>> FilterPredicate visit(Operators.Gt<T> gt) {
+			if (needToBeNotNull(gt, gt.getColumn().getColumnPath())) {
+				return andNotNull(gt, gt.getColumn());
+			}
+
+			return gt;
+		}
+
+		@Override
+		public <T extends Comparable<T>> FilterPredicate visit(Operators.GtEq<T> gtEq) {
+			if (needToBeNotNull(gtEq, gtEq.getColumn().getColumnPath())) {
+				return andNotNull(gtEq, gtEq.getColumn());
+			}
+
+			return gtEq;
+		}
+
+		// Check whether we need to add a `not null` predicate to a existing column predicate.
+		// Return false only if there exists an `and` brother which is `is not null` already.
+		boolean needToBeNotNull(FilterPredicate predicate, ColumnPath columnPath) {
+			if (!(parent instanceof Operators.And)) {
+				return true;
+			}
+
+			boolean ret = true;
+
+			Operators.And andParent = (Operators.And) parent;
+			FilterPredicate brother =
+				andParent.getLeft() == predicate ? andParent.getRight() : andParent.getLeft();
+			if (brother instanceof Operators.NotEq) {
+				Operators.NotEq notEq = (Operators.NotEq) brother;
+
+				if (notEq.getColumn().getColumnPath().equals(columnPath)
+					&& notEq.getValue() == null) {
+					// already has an `and` brother which means `is not null` for the same column
+					ret = false;
+				}
+			}
+
+			return ret;
+		}
+
+		@Nullable
+		private FilterPredicate notEquals(Tuple2<Operators.Column, Comparable> columnPair) {
+			if (columnPair.f0 instanceof Operators.IntColumn) {
+				return FilterApi.notEq(
+					(Operators.IntColumn) columnPair.f0,
+					(Integer) columnPair.f1);
+			} else if (columnPair.f0 instanceof Operators.LongColumn) {
+				return FilterApi.notEq((Operators.LongColumn) columnPair.f0, (Long) columnPair.f1);
+			} else if (columnPair.f0 instanceof Operators.DoubleColumn) {
+				return FilterApi.notEq(
+					(Operators.DoubleColumn) columnPair.f0,
+					(Double) columnPair.f1);
+			} else if (columnPair.f0 instanceof Operators.FloatColumn) {
+				return FilterApi.notEq(
+					(Operators.FloatColumn) columnPair.f0,
+					(Float) columnPair.f1);
+			} else if (columnPair.f0 instanceof Operators.BooleanColumn) {
+				return FilterApi.notEq(
+					(Operators.BooleanColumn) columnPair.f0,
+					(Boolean) columnPair.f1);
+			} else if (columnPair.f0 instanceof Operators.BinaryColumn) {
+				return FilterApi.notEq(
+					(Operators.BinaryColumn) columnPair.f0,
+					(Binary) columnPair.f1);
+			}
+
+			return null;
+		}
+
+		private FilterPredicate andNotNull(FilterPredicate predicate, Operators.Column column) {
+			FilterPredicate notEqNull = notEquals(new Tuple2<>(column, null));
+			if (notEqNull != null) {
+				return FilterApi.and(notEqNull, predicate);
+			}
+
+			return predicate;
+		}
+
+		@Override
+		public FilterPredicate visit(Operators.And and) {
+			parent = and;
+			FilterPredicate left = and.getLeft().accept(this);
+
+			parent = and;
+			FilterPredicate right = and.getRight().accept(this);
+
+			return FilterApi.and(left, right);
+		}
+
+		@Override
+		public FilterPredicate visit(Operators.Or or) {
+			parent = or;
+			FilterPredicate left = or.getLeft().accept(this);
+
+			parent = or;
+			FilterPredicate right = or.getRight().accept(this);
+
+			return FilterApi.or(left, right);
+		}
+
+		@Override
+		public FilterPredicate visit(Operators.Not not) {
+			parent = not;
+
+			FilterPredicate predicate = not.getPredicate().accept(this);
+
+			return FilterApi.not(predicate);
+		}
+
+		@Override
+		public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> FilterPredicate visit(
+			Operators.UserDefined<T, U> userDefined) {
+			return userDefined;
+		}
+
+		@Override
+		public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> FilterPredicate visit(
+			Operators.LogicalNotUserDefined<T, U> logicalNotUserDefined) {
+			return logicalNotUserDefined;
+		}
+	}
+}
diff --git a/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java b/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java
index 6c6d2a44b1..158056302e 100644
--- a/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java
+++ b/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java
@@ -246,6 +246,7 @@ public abstract class FileSystem {
 			.put("s3a", "flink-s3-fs-hadoop")
 			.put("s3p", "flink-s3-fs-presto")
 			.put("swift", "flink-swift-fs-hadoop")
+			.put("cachedFs", "flink-cached-fs-hadoop")
 			// mapr deliberately omitted for now (no dedicated plugin)
 			.build();
 
diff --git a/flink-filesystems/flink-cached-fs-hadoop/pom.xml b/flink-filesystems/flink-cached-fs-hadoop/pom.xml
new file mode 100644
index 0000000000..7cf32fe281
--- /dev/null
+++ b/flink-filesystems/flink-cached-fs-hadoop/pom.xml
@@ -0,0 +1,188 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+		 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+		 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+	<parent>
+		<artifactId>flink-filesystems</artifactId>
+		<groupId>org.apache.flink</groupId>
+		<version>1.12.0</version>
+	</parent>
+	<modelVersion>4.0.0</modelVersion>
+
+	<artifactId>flink-cached-fs-hadoop</artifactId>
+
+	<dependencies>
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-core</artifactId>
+			<version>${project.version}</version>
+			<scope>provided</scope>
+		</dependency>
+
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-hadoop-fs</artifactId>
+			<version>${project.version}</version>
+		</dependency>
+
+		<dependency>
+			<groupId>org.apache.hadoop</groupId>
+			<artifactId>hadoop-common</artifactId>
+			<optional>true</optional>
+		</dependency>
+
+		<dependency>
+			<groupId>org.apache.hadoop</groupId>
+			<artifactId>hadoop-hdfs</artifactId>
+			<optional>true</optional>
+		</dependency>
+
+		<dependency>
+			<groupId>com.intel.oap</groupId>
+			<artifactId>hcfs-sql-ds-cache</artifactId>
+			<version>1.1.0</version>
+		</dependency>
+
+		<!-- Dependencies for test suite -->
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-core</artifactId>
+			<version>${project.version}</version>
+			<scope>test</scope>
+			<type>test-jar</type>
+		</dependency>
+
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-hadoop-fs</artifactId>
+			<version>${project.version}</version>
+			<scope>test</scope>
+			<type>test-jar</type>
+		</dependency>
+
+		<dependency>
+			<groupId>org.apache.hadoop</groupId>
+			<artifactId>hadoop-common</artifactId>
+			<version>${hadoop.version}</version>
+			<scope>test</scope>
+			<type>test-jar</type>
+			<exclusions>
+				<exclusion>
+					<groupId>log4j</groupId>
+					<artifactId>log4j</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>org.slf4j</groupId>
+					<artifactId>slf4j-log4j12</artifactId>
+				</exclusion>
+			</exclusions>
+		</dependency>
+
+		<dependency>
+			<groupId>org.apache.hadoop</groupId>
+			<artifactId>hadoop-hdfs</artifactId>
+			<version>${hadoop.version}</version>
+			<scope>test</scope>
+			<type>test-jar</type>
+			<exclusions>
+				<exclusion>
+					<groupId>log4j</groupId>
+					<artifactId>log4j</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>org.slf4j</groupId>
+					<artifactId>slf4j-log4j12</artifactId>
+				</exclusion>
+			</exclusions>
+		</dependency>
+	</dependencies>
+
+	<build>
+		<plugins>
+
+			<!-- Relocate all related classes -->
+			<plugin>
+				<groupId>org.apache.maven.plugins</groupId>
+				<artifactId>maven-shade-plugin</artifactId>
+				<executions>
+					<execution>
+						<id>shade-flink</id>
+						<phase>package</phase>
+						<goals>
+							<goal>shade</goal>
+						</goals>
+						<configuration>
+							<shadeTestJar>false</shadeTestJar>
+							<artifactSet>
+								<includes>
+									<include>*:*</include>
+								</includes>
+							</artifactSet>
+							<relocations>
+								<!-- shade Flink's Hadoop FS adapter classes, forces plugin classloader for them -->
+								<relocation>
+									<pattern>org.apache.flink.runtime.fs.hdfs</pattern>
+									<shadedPattern>org.apache.flink.fs.pmemhadoop.shaded.org.apache.flink.runtime.fs.hdfs</shadedPattern>
+								</relocation>
+								<relocation>
+									<pattern>org.apache.flink.runtime.util</pattern>
+									<shadedPattern>org.apache.flink.fs.pmemhadoop.shaded.org.apache.flink.runtime.util</shadedPattern>
+									<includes>
+										<include>org.apache.flink.runtime.util.**Hadoop*</include>
+									</includes>
+								</relocation>
+							</relocations>
+							<filters>
+								<filter>
+									<artifact>*</artifact>
+									<excludes>
+										<exclude>.gitkeep</exclude>
+										<exclude>mime.types</exclude>
+										<exclude>mozilla/**</exclude>
+										<exclude>META-INF/maven/**</exclude>
+									</excludes>
+								</filter>
+							</filters>
+						</configuration>
+					</execution>
+				</executions>
+			</plugin>
+			<plugin>
+				<groupId>org.apache.maven.plugins</groupId>
+				<artifactId>maven-jar-plugin</artifactId>
+				<executions>
+					<execution>
+						<goals>
+							<goal>test-jar</goal>
+						</goals>
+					</execution>
+				</executions>
+			</plugin>
+			<plugin>
+				<groupId>org.apache.maven.plugins</groupId>
+				<artifactId>maven-surefire-plugin</artifactId>
+				<configuration>
+					<forkMode>once</forkMode>
+					<workingDirectory>target</workingDirectory>
+					<argLine>-Djava.library.path=${basedir}</argLine>
+				</configuration>
+			</plugin>
+		</plugins>
+	</build>
+</project>
diff --git a/flink-filesystems/flink-cached-fs-hadoop/src/main/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemFactory.java b/flink-filesystems/flink-cached-fs-hadoop/src/main/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemFactory.java
new file mode 100644
index 0000000000..0e8d3cff9d
--- /dev/null
+++ b/flink-filesystems/flink-cached-fs-hadoop/src/main/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemFactory.java
@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.fs.pmemhadoop;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.fs.FileSystem;
+import org.apache.flink.core.fs.FileSystemFactory;
+import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;
+
+import com.intel.oap.fs.hadoop.cachedfs.CachedFileSystem;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.net.URI;
+
+
+/**
+ * Simple factory for the PMem Cached file system.
+ */
+public class CachedFileSystemFactory implements FileSystemFactory {
+	private static final Logger LOG = LoggerFactory.getLogger(CachedFileSystemFactory.class);
+
+	private Configuration flinkConfig;
+
+	private org.apache.hadoop.conf.Configuration hadoopConfig;
+
+	private static final String[] FLINK_CONFIG_PREFIXES = { "fs.cachedFs."};
+
+	@Override
+	public String getScheme() {
+		return "cachedFs";
+	}
+
+	@Override
+	public void configure(Configuration config) {
+		flinkConfig = config;
+		hadoopConfig = null;
+	}
+
+	@Override
+	public FileSystem create(URI fsUri) throws IOException {
+		LOG.info("create cachedFs instance for URI: {}", fsUri);
+
+		this.hadoopConfig = getHadoopConfiguration();
+
+		final String scheme = fsUri.getScheme();
+		final String authority = fsUri.getAuthority();
+
+		if (scheme == null && authority == null) {
+			fsUri = org.apache.hadoop.fs.FileSystem.getDefaultUri(hadoopConfig);
+		} else if (scheme != null && authority == null) {
+			URI defaultUri = org.apache.hadoop.fs.FileSystem.getDefaultUri(hadoopConfig);
+			if (scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null) {
+				fsUri = defaultUri;
+			}
+		}
+
+		final CachedFileSystem fs = new CachedFileSystem();
+		fs.initialize(fsUri, hadoopConfig);
+		return new HadoopFileSystem(fs);
+	}
+
+	@VisibleForTesting
+	org.apache.hadoop.conf.Configuration getHadoopConfiguration() {
+		org.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration();
+		if (flinkConfig == null) {
+			return conf;
+		}
+
+		// read all configuration with prefix 'FLINK_CONFIG_PREFIXES'
+		for (String key : flinkConfig.keySet()) {
+			for (String prefix : FLINK_CONFIG_PREFIXES) {
+				if (key.startsWith(prefix)) {
+					String value = flinkConfig.getString(key, null);
+					conf.set(key, value);
+
+					LOG.debug("Adding Flink config entry for {} as {} to Hadoop config", key, conf.get(key));
+				}
+			}
+		}
+		return conf;
+	}
+}
diff --git a/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/NOTICE b/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/NOTICE
new file mode 100644
index 0000000000..eff73c18a7
--- /dev/null
+++ b/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/NOTICE
@@ -0,0 +1,21 @@
+flink-cached-fs-hadoop
+Copyright 2014-2020 The Apache Software Foundation
+
+This project includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+This project bundles the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)
+
+- com.intel.oap:pmem-cached-hcfs:jar:0.0.1
+- commons-codec:commons-codec:jar:1.10
+- commons-logging:commons-logging:jar:1.1.3
+- org.apache.hadoop:hadoop-aliyun:jar:3.1.0
+- org.apache.httpcomponents:httpclient:jar:4.5.3
+- org.apache.httpcomponents:httpcore:jar:4.4.6
+- org.codehaus.jettison:jettison:jar:1.1
+- stax:stax-api:1.0.1
+
+This project bundles the following dependencies under the JDOM license.
+See bundled license files for details.
+
+- org.jdom:jdom:1.1
diff --git a/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/services/org.apache.flink.core.fs.FileSystemFactory b/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/services/org.apache.flink.core.fs.FileSystemFactory
new file mode 100644
index 0000000000..3896248fa5
--- /dev/null
+++ b/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/META-INF/services/org.apache.flink.core.fs.FileSystemFactory
@@ -0,0 +1,16 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+org.apache.flink.fs.pmemhadoop.CachedFileSystemFactory
diff --git a/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/licenses/LICENSE.jdom b/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/licenses/LICENSE.jdom
new file mode 100644
index 0000000000..b420aeb000
--- /dev/null
+++ b/flink-filesystems/flink-cached-fs-hadoop/src/main/resources/licenses/LICENSE.jdom
@@ -0,0 +1,51 @@
+ Copyright (C) 2000-2004 Jason Hunter & Brett McLaughlin.
+ All rights reserved.
+
+This project bundles the following dependencies under the following license.
+
+ Redistribution and use in source and binary forms, with or without
+ modification, are permitted provided that the following conditions
+ are met:
+
+ 1. Redistributions of source code must retain the above copyright
+    notice, this list of conditions, and the following disclaimer.
+
+ 2. Redistributions in binary form must reproduce the above copyright
+    notice, this list of conditions, and the disclaimer that follows
+    these conditions in the documentation and/or other materials
+    provided with the distribution.
+
+ 3. The name "JDOM" must not be used to endorse or promote products
+    derived from this software without prior written permission.  For
+    written permission, please contact <request_AT_jdom_DOT_org>.
+
+ 4. Products derived from this software may not be called "JDOM", nor
+    may "JDOM" appear in their name, without prior written permission
+    from the JDOM Project Management <request_AT_jdom_DOT_org>.
+
+ In addition, we request (but do not require) that you include in the
+ end-user documentation provided with the redistribution and/or in the
+ software itself an acknowledgement equivalent to the following:
+     "This product includes software developed by the
+      JDOM Project (http://www.jdom.org/)."
+ Alternatively, the acknowledgment may be graphical using the logos
+ available at http://www.jdom.org/images/logos.
+
+ THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED
+ WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ DISCLAIMED.  IN NO EVENT SHALL THE JDOM AUTHORS OR THE PROJECT
+ CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
+ USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
+ OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ SUCH DAMAGE.
+
+ This software consists of voluntary contributions made by many
+ individuals on behalf of the JDOM Project and was originally
+ created by Jason Hunter <jhunter_AT_jdom_DOT_org> and
+ Brett McLaughlin <brett_AT_jdom_DOT_org>.  For more information
+ on the JDOM Project, please see <http://www.jdom.org/>.
diff --git a/flink-filesystems/flink-cached-fs-hadoop/src/test/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemTest.java b/flink-filesystems/flink-cached-fs-hadoop/src/test/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemTest.java
new file mode 100644
index 0000000000..d2ddd562ac
--- /dev/null
+++ b/flink-filesystems/flink-cached-fs-hadoop/src/test/java/org/apache/flink/fs/pmemhadoop/CachedFileSystemTest.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.fs.pmemhadoop;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.fs.FileSystem;
+import org.apache.flink.core.fs.Path;
+import org.apache.flink.runtime.fs.hdfs.AbstractHadoopFileSystemITTest;
+
+import org.junit.BeforeClass;
+
+import java.io.IOException;
+import java.net.URI;
+
+/**
+ * Unit test.
+ */
+public class CachedFileSystemTest extends AbstractHadoopFileSystemITTest {
+	private static final String TEST_DATA_DIR = "tests-0543f8ad-86ae-4016-8627-4547eb8b750f";
+
+	@BeforeClass
+	public static void setup() throws IOException {
+		final Configuration conf = new Configuration();
+
+		FileSystem.initialize(conf);
+		URI hdfsDefaultUri = new Path("hdfs:///").getFileSystem().getUri();
+		String cachedFsPath = String.format("cachedFs://%s:%s/%s", hdfsDefaultUri.getHost(), hdfsDefaultUri.getPort(), TEST_DATA_DIR);
+
+		basePath = new Path(cachedFsPath);
+		fs = basePath.getFileSystem();
+		consistencyToleranceNS = 0;
+	}
+
+//	@Test
+//	public void testWorkingWithHdfsScheme() throws Exception {
+//		final Configuration conf = new Configuration();
+//		CachedFileSystemFactory factory = new CachedFileSystemFactory();
+//		factory.configure(conf);
+//
+//		// create a FileSystem with hdfs scheme
+//		URI hdfsDefaultUri = new Path("hdfs:///").getFileSystem().getUri();
+//		String path = String.format("%s/%s", hdfsDefaultUri.toString(), TEST_DATA_DIR);
+//		basePath = new Path(path);
+//		fs = factory.create(URI.create(path));
+//
+//		// run test cases
+//		this.testSimpleFileWriteAndRead();
+//		this.testDirectoryListing();
+//	}
+}
diff --git a/flink-filesystems/flink-cached-fs-hadoop/src/test/resources/log4j2-test.properties b/flink-filesystems/flink-cached-fs-hadoop/src/test/resources/log4j2-test.properties
new file mode 100644
index 0000000000..2d91a377a0
--- /dev/null
+++ b/flink-filesystems/flink-cached-fs-hadoop/src/test/resources/log4j2-test.properties
@@ -0,0 +1,8 @@
+rootLogger.level=INFO
+rootLogger.appenderRef.test.ref = TestLogger
+
+appender.testlogger.name = TestLogger
+appender.testlogger.type = CONSOLE
+appender.testlogger.target = SYSTEM_ERR
+appender.testlogger.layout.type = PatternLayout
+appender.testlogger.layout.pattern = %-4r [%t] %-5p %c %x - %m%n
diff --git a/flink-filesystems/pom.xml b/flink-filesystems/pom.xml
index bc02a6fe06..37b37ebb4f 100644
--- a/flink-filesystems/pom.xml
+++ b/flink-filesystems/pom.xml
@@ -48,6 +48,7 @@ under the License.
 		<module>flink-swift-fs-hadoop</module>
 		<module>flink-oss-fs-hadoop</module>
 		<module>flink-azure-fs-hadoop</module>
+		<module>flink-cached-fs-hadoop</module>
 	</modules>
 
 	<!-- Common dependency setup for all filesystems -->
diff --git a/flink-formats/flink-parquet/pom.xml b/flink-formats/flink-parquet/pom.xml
index 769317926e..7c31320481 100644
--- a/flink-formats/flink-parquet/pom.xml
+++ b/flink-formats/flink-parquet/pom.xml
@@ -124,6 +124,22 @@ under the License.
 			<scope>provided</scope>
 		</dependency>
 
+		<dependency>
+			<groupId>com.intel.oap</groupId>
+			<artifactId>ape-flink</artifactId>
+			<version>1.1.0-SNAPSHOT</version>
+			<exclusions>
+				<exclusion>
+					<groupId>org.slf4j</groupId>
+					<artifactId>slf4j-log4j12</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>log4j</groupId>
+					<artifactId>log4j</artifactId>
+				</exclusion>
+			</exclusions>
+		</dependency>
+
 		<!-- Optional Parquet Builders for Formats like Avro, Protobuf, Thrift -->
 
 		<dependency>
diff --git a/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetColumnarRowInputFormat.java b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetColumnarRowInputFormat.java
index 19c25d8a50..752e78268d 100644
--- a/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetColumnarRowInputFormat.java
+++ b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetColumnarRowInputFormat.java
@@ -22,6 +22,7 @@ import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.connector.file.src.FileSourceSplit;
 import org.apache.flink.connector.file.src.util.Pool;
 import org.apache.flink.core.fs.Path;
+import org.apache.flink.formats.parquet.ape.CustomizedParquetVectorizedInputFormat;
 import org.apache.flink.formats.parquet.utils.SerializableConfiguration;
 import org.apache.flink.formats.parquet.vector.ColumnBatchFactory;
 import org.apache.flink.table.data.ColumnarRowData;
@@ -46,7 +47,7 @@ import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.cre
  * Using {@link ColumnarRowData} to provide a row view of column batch.
  */
 public class ParquetColumnarRowInputFormat<SplitT extends FileSourceSplit> extends
-		ParquetVectorizedInputFormat<RowData, SplitT> {
+	CustomizedParquetVectorizedInputFormat<RowData, SplitT> {
 
 	private static final long serialVersionUID = 1L;
 
diff --git a/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/ConfKeys.java b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/ConfKeys.java
new file mode 100644
index 0000000000..fe71b120a6
--- /dev/null
+++ b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/ConfKeys.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.formats.parquet.ape;
+
+/**
+ * Config keys used in ape packages.
+ */
+public class ConfKeys {
+	// use native parquet reader or not.
+	public static final String USE_NATIVE_PARQUET_READER = "table.exec.hive.parquet-native-reader";
+
+	// mode of native parquet reader.
+	public static final String NATIVE_PARQUET_READER_MODE = "table.exec.hive.parquet-native-reader.mode";
+}
diff --git a/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/CustomizedParquetVectorizedInputFormat.java b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/CustomizedParquetVectorizedInputFormat.java
new file mode 100644
index 0000000000..78c739a463
--- /dev/null
+++ b/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ape/CustomizedParquetVectorizedInputFormat.java
@@ -0,0 +1,602 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.formats.parquet.ape;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.connector.base.source.reader.SourceReaderOptions;
+import org.apache.flink.connector.file.src.FileSourceSplit;
+import org.apache.flink.connector.file.src.reader.BulkFormat;
+import org.apache.flink.connector.file.src.util.CheckpointedPosition;
+import org.apache.flink.connector.file.src.util.Pool;
+import org.apache.flink.core.fs.Path;
+import org.apache.flink.formats.parquet.utils.ParquetNativeRecordReaderWrapper;
+import org.apache.flink.formats.parquet.utils.ParquetRecordReaderWrapper;
+import org.apache.flink.formats.parquet.utils.ParquetRemoteRecordReaderWrapper;
+import org.apache.flink.formats.parquet.utils.SerializableConfiguration;
+import org.apache.flink.formats.parquet.vector.ColumnBatchFactory;
+import org.apache.flink.formats.parquet.vector.ParquetDecimalVector;
+import org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader;
+import org.apache.flink.formats.parquet.vector.reader.ColumnReader;
+import org.apache.flink.table.data.vector.ColumnVector;
+import org.apache.flink.table.data.vector.VectorizedColumnBatch;
+import org.apache.flink.table.data.vector.writable.WritableColumnVector;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.utils.ape.AggregateExprs;
+import org.apache.flink.util.FlinkRuntimeException;
+import org.apache.flink.util.Preconditions;
+
+import com.fasterxml.jackson.annotation.JsonInclude;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.SerializationFeature;
+import org.apache.parquet.column.ColumnDescriptor;
+import org.apache.parquet.column.page.PageReadStore;
+import org.apache.parquet.filter2.compat.FilterCompat;
+import org.apache.parquet.filter2.predicate.FilterPredicate;
+import org.apache.parquet.hadoop.ParquetFileReader;
+import org.apache.parquet.hadoop.metadata.BlockMetaData;
+import org.apache.parquet.hadoop.metadata.ParquetMetadata;
+import org.apache.parquet.hadoop.util.SerializationUtil;
+import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.MessageType;
+import org.apache.parquet.schema.Type;
+import org.apache.parquet.schema.Types;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.annotation.Nullable;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader;
+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector;
+import static org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups;
+import static org.apache.parquet.format.converter.ParquetMetadataConverter.range;
+import static org.apache.parquet.hadoop.ParquetFileReader.readFooter;
+import static org.apache.parquet.hadoop.ParquetInputFormat.getFilter;
+
+/**
+ * Parquet {@link BulkFormat} that reads data from the file to
+ * {@link VectorizedColumnBatch} in vectorized mode.
+ */
+public abstract class CustomizedParquetVectorizedInputFormat<T, SplitT extends FileSourceSplit>
+	implements BulkFormat<T, SplitT> {
+
+	private static final long serialVersionUID = 1L;
+
+	private final SerializableConfiguration hadoopConfig;
+	private final String[] projectedFields;
+	private final LogicalType[] projectedTypes;
+	private final RowType projectedType;
+	private final ColumnBatchFactory<SplitT> batchFactory;
+	private final int batchSize;
+	private final boolean isCaseSensitive;
+	private final boolean isUtcTimestamp;
+
+	private final boolean useNativeParquetReader;
+	private static final String APE_READER_REMOTE_MODE = "remote";
+	private boolean aggregatePushedDown = false;
+	private ParquetRecordReaderWrapper nativeReaderWrapper;
+
+	private static final Logger LOG = LoggerFactory.getLogger(CustomizedParquetVectorizedInputFormat.class);
+
+	public CustomizedParquetVectorizedInputFormat(
+		SerializableConfiguration hadoopConfig,
+		RowType projectedType,
+		ColumnBatchFactory<SplitT> batchFactory,
+		int batchSize,
+		boolean isUtcTimestamp,
+		boolean isCaseSensitive) {
+		this.hadoopConfig = hadoopConfig;
+		this.projectedFields = projectedType.getFieldNames().toArray(new String[0]);
+		this.projectedTypes = projectedType.getChildren().toArray(new LogicalType[0]);
+		this.batchFactory = batchFactory;
+		this.batchSize = batchSize;
+		this.isUtcTimestamp = isUtcTimestamp;
+		this.isCaseSensitive = isCaseSensitive;
+		this.projectedType = projectedType;
+
+		useNativeParquetReader = hadoopConfig.conf().getBoolean(
+			ConfKeys.USE_NATIVE_PARQUET_READER, false);
+	}
+
+	@Override
+	public ParquetReader createReader(
+		final Configuration config,
+		final SplitT split) throws IOException {
+
+		final Path filePath = split.path();
+		final long splitOffset = split.offset();
+		final long splitLength = split.length();
+
+		org.apache.hadoop.fs.Path hadoopPath = new org.apache.hadoop.fs.Path(filePath.toUri());
+		ParquetMetadata footer = readFooter(hadoopConfig.conf(), hadoopPath,
+			range(splitOffset, splitOffset + splitLength));
+		MessageType fileSchema = footer.getFileMetaData().getSchema();
+		FilterCompat.Filter filter = getFilter(hadoopConfig.conf());
+		List<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);
+
+		// get filters pushed by HiveTableSource
+		FilterPredicate predicate = SerializationUtil.readObjectFromConfAsBase64(
+			"parquet.private.native.reader.filter.predicate", hadoopConfig.conf());
+
+		LOG.info("deserialized predicate toString: {}", predicate);
+		LOG.debug(
+			"parquet.private.native.reader.filter.predicate.human.readable: {}",
+			hadoopConfig
+				.conf()
+				.get("parquet.private.native.reader.filter.predicate.human.readable"));
+
+		// get aggregate pushed by HiveTableSource
+		AggregateExprs agg = SerializationUtil.readObjectFromConfAsBase64(
+			"parquet.private.native.reader.agg.expressions", hadoopConfig.conf());
+		String aggStr = null;
+		if (agg != null) {
+			aggStr = new ObjectMapper()
+				.setSerializationInclusion(JsonInclude.Include.NON_NULL)
+				.configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, false)
+				.writeValueAsString(agg);
+			LOG.info("aggregate pushing down: {}", aggStr);
+		}
+
+		MessageType requestedSchema = null;
+		ParquetFileReader reader = null;
+		long totalRowCount = 0;
+
+		// schema can be clipped from file schema when no aggregations are pushed down
+		if (aggStr == null) {
+			requestedSchema = clipParquetSchema(fileSchema);
+			reader = new ParquetFileReader(hadoopConfig.conf(),
+				footer.getFileMetaData(),
+				hadoopPath,
+				blocks,
+				requestedSchema.getColumns());
+
+			totalRowCount = 0;
+			for (BlockMetaData block : blocks) {
+				totalRowCount += block.getRowCount();
+			}
+
+			checkSchema(fileSchema, requestedSchema);
+		}
+
+		// native parquet reader is optional
+		if (useNativeParquetReader) {
+			String readerMode = hadoopConfig.conf().get(ConfKeys.NATIVE_PARQUET_READER_MODE, "");
+			if (!readerMode.equals(APE_READER_REMOTE_MODE)) {
+				nativeReaderWrapper = new ParquetNativeRecordReaderWrapper(batchSize);
+			} else {
+				nativeReaderWrapper = new ParquetRemoteRecordReaderWrapper(batchSize);
+			}
+
+			nativeReaderWrapper.initialize(hadoopConfig.conf(), projectedType, split, predicate, aggStr);
+			if (aggStr != null) {
+				aggregatePushedDown = true;
+			}
+		}
+
+		final int numBatchesToCirculate = config.getInteger(SourceReaderOptions.ELEMENT_QUEUE_CAPACITY);
+		final Pool<ParquetReaderBatch<T>> poolOfBatches = createPoolOfBatches(split,
+			requestedSchema,
+			numBatchesToCirculate);
+
+		return new ParquetReader(
+			reader,
+			requestedSchema,
+			totalRowCount,
+			poolOfBatches,
+			nativeReaderWrapper);
+	}
+
+	@Override
+	public ParquetReader restoreReader(
+		final Configuration config,
+		final SplitT split) throws IOException {
+
+		assert split.getReaderPosition().isPresent();
+		final CheckpointedPosition checkpointedPosition = split.getReaderPosition().get();
+
+		Preconditions.checkArgument(
+			checkpointedPosition.getOffset() == CheckpointedPosition.NO_OFFSET,
+			"The offset of CheckpointedPosition should always be NO_OFFSET");
+		ParquetReader reader = createReader(config, split);
+
+		if (!aggregatePushedDown) {
+			reader.seek(checkpointedPosition.getRecordsAfterOffset());
+		}
+		return reader;
+	}
+
+	@Override
+	public boolean isSplittable() {
+		return true;
+	}
+
+	/**
+	 * Clips `parquetSchema` according to `fieldNames`.
+	 */
+	private MessageType clipParquetSchema(GroupType parquetSchema) {
+		Type[] types = new Type[projectedFields.length];
+		if (isCaseSensitive) {
+			for (int i = 0; i < projectedFields.length; ++i) {
+				String fieldName = projectedFields[i];
+				if (parquetSchema.getFieldIndex(fieldName) < 0) {
+					throw new IllegalArgumentException(fieldName + " does not exist");
+				}
+				types[i] = parquetSchema.getType(fieldName);
+			}
+		} else {
+			Map<String, Type> caseInsensitiveFieldMap = new HashMap<>();
+			for (Type type : parquetSchema.getFields()) {
+				caseInsensitiveFieldMap.compute(
+					type.getName().toLowerCase(Locale.ROOT),
+					(key, previousType) -> {
+						if (previousType != null) {
+							throw new FlinkRuntimeException(
+								"Parquet with case insensitive mode should have no duplicate key: "
+									+ key);
+						}
+						return type;
+					});
+			}
+			for (int i = 0; i < projectedFields.length; ++i) {
+				Type type = caseInsensitiveFieldMap.get(projectedFields[i].toLowerCase(Locale.ROOT));
+				if (type == null) {
+					throw new IllegalArgumentException(projectedFields[i] + " does not exist");
+				}
+				// TODO clip for array,map,row types.
+				types[i] = type;
+			}
+		}
+
+		return Types.buildMessage().addFields(types).named("flink-parquet");
+	}
+
+	private void checkSchema(MessageType fileSchema, MessageType requestedSchema)
+		throws IOException, UnsupportedOperationException {
+		if (projectedFields.length != requestedSchema.getFieldCount()) {
+			throw new RuntimeException(
+				"The quality of field type is incompatible with the request schema!");
+		}
+
+		/*
+		 * Check that the requested schema is supported.
+		 */
+		for (int i = 0; i < requestedSchema.getFieldCount(); ++i) {
+			Type t = requestedSchema.getFields().get(i);
+			if (!t.isPrimitive() || t.isRepetition(Type.Repetition.REPEATED)) {
+				throw new UnsupportedOperationException("Complex types not supported.");
+			}
+
+			String[] colPath = requestedSchema.getPaths().get(i);
+			if (fileSchema.containsPath(colPath)) {
+				ColumnDescriptor fd = fileSchema.getColumnDescription(colPath);
+				if (!fd.equals(requestedSchema.getColumns().get(i))) {
+					throw new UnsupportedOperationException("Schema evolution not supported.");
+				}
+			} else {
+				if (requestedSchema.getColumns().get(i).getMaxDefinitionLevel() == 0) {
+					// Column is missing in data but the required data is non-nullable. This file is
+					// invalid.
+					throw new IOException("Required column is missing in data file. Col: "
+						+ Arrays.toString(colPath));
+				}
+			}
+		}
+	}
+
+	private Pool<ParquetReaderBatch<T>> createPoolOfBatches(
+		SplitT split,
+		MessageType requestedSchema,
+		int numBatches) {
+		final Pool<ParquetReaderBatch<T>> pool = new Pool<>(numBatches);
+
+		for (int i = 0; i < numBatches; i++) {
+			pool.add(createReaderBatch(split, requestedSchema, pool.recycler()));
+		}
+
+		return pool;
+	}
+
+	private ParquetReaderBatch<T> createReaderBatch(
+		SplitT split, MessageType requestedSchema,
+		Pool.Recycler<ParquetReaderBatch<T>> recycler) {
+
+		WritableColumnVector[] writableVectors;
+
+		if (!useNativeParquetReader) {
+			writableVectors = createWritableVectors(requestedSchema);
+		} else {
+			// native parquet reader is optional
+			writableVectors = nativeReaderWrapper.initBatch(
+				requestedSchema,
+				batchSize,
+				projectedType);
+		}
+
+		VectorizedColumnBatch columnarBatch = batchFactory.create(
+			split,
+			createReadableVectors(writableVectors));
+		return createReaderBatch(writableVectors, columnarBatch, recycler);
+	}
+
+	private WritableColumnVector[] createWritableVectors(MessageType requestedSchema) {
+		WritableColumnVector[] columns = new WritableColumnVector[projectedTypes.length];
+		for (int i = 0; i < projectedTypes.length; i++) {
+			columns[i] = createWritableColumnVector(
+				batchSize,
+				projectedTypes[i],
+				requestedSchema.getColumns().get(i).getPrimitiveType());
+		}
+		return columns;
+	}
+
+	/**
+	 * Create readable vectors from writable vectors. Especially for decimal, see
+	 * {@link ParquetDecimalVector}.
+	 */
+	private ColumnVector[] createReadableVectors(WritableColumnVector[] writableVectors) {
+		ColumnVector[] vectors = new ColumnVector[writableVectors.length];
+		for (int i = 0; i < writableVectors.length; i++) {
+			vectors[i] = projectedTypes[i].getTypeRoot() == LogicalTypeRoot.DECIMAL
+				? new ParquetDecimalVector(writableVectors[i])
+				: writableVectors[i];
+		}
+		return vectors;
+	}
+
+	private class ParquetReader implements Reader<T> {
+
+		private ParquetFileReader reader;
+
+		private final MessageType requestedSchema;
+
+		/**
+		 * The total number of rows this RecordReader will eventually read. The sum of
+		 * the rows of all the row groups.
+		 */
+		private final long totalRowCount;
+
+		private final Pool<ParquetReaderBatch<T>> pool;
+
+		private ParquetRecordReaderWrapper nativeReaderWrapper;
+
+		/**
+		 * The number of rows that have been returned.
+		 */
+		private long rowsReturned;
+
+		/**
+		 * The number of rows that have been reading, including the current in flight
+		 * row group.
+		 */
+		private long totalCountLoadedSoFar;
+
+		/**
+		 * For each request column, the reader to read this column. This is NULL if this column is
+		 * missing from the file, in which case we populate the attribute with NULL.
+		 */
+		@SuppressWarnings("rawtypes")
+		private ColumnReader[] columnReaders;
+
+		/**
+		 * For each request column, the reader to read this column. This is NULL if this
+		 * column is missing from the file, in which case we populate the attribute with
+		 * NULL.
+		 */
+		private long recordsToSkip;
+
+		private ParquetReader(
+			ParquetFileReader reader,
+			MessageType requestedSchema,
+			long totalRowCount,
+			Pool<ParquetReaderBatch<T>> pool,
+			ParquetRecordReaderWrapper nativeReaderWrapper) {
+			this.reader = reader;
+			this.requestedSchema = requestedSchema;
+			this.totalRowCount = totalRowCount;
+			this.pool = pool;
+			this.rowsReturned = 0;
+			this.totalCountLoadedSoFar = 0;
+			this.recordsToSkip = 0;
+			this.nativeReaderWrapper = nativeReaderWrapper;
+		}
+
+		@Nullable
+		@Override
+		public RecordIterator<T> readBatch() throws IOException {
+			final ParquetReaderBatch<T> batch = getCachedEntry();
+
+			final long rowsReturnedBefore = rowsReturned;
+			if (!nextBatch(batch)) {
+				batch.recycle();
+				return null;
+			}
+
+			final RecordIterator<T> records = batch.convertAndGetIterator(rowsReturnedBefore);
+
+			// and is not interpreted as end-of-input or anything
+			skipRecord(records);
+			return records;
+		}
+
+		/**
+		 * Advances to the next batch of rows. Returns false if there are no more.
+		 */
+		private boolean nextBatch(ParquetReaderBatch<T> batch) throws IOException {
+			for (WritableColumnVector v : batch.writableVectors) {
+				v.reset();
+			}
+
+			batch.columnarBatch.setNumRows(0);
+
+			int rowsRead;
+			if (nativeReaderWrapper != null) {
+				// Filters and aggregate expressions may be pushed down to native reader.
+				// So the `totalRowCount` cannot be used to stop data loading
+				if (!nativeReaderWrapper.nextBatch(batch.writableVectors)) {
+					return false;
+				}
+
+				rowsRead = nativeReaderWrapper.getRowsRead();
+			} else {
+				if (rowsReturned >= totalRowCount) {
+					return false;
+				}
+
+				if (rowsReturned == totalCountLoadedSoFar) {
+					readNextRowGroup();
+				}
+
+				rowsRead = (int) Math.min(batchSize, totalCountLoadedSoFar - rowsReturned);
+				for (int i = 0; i < columnReaders.length; ++i) {
+					//noinspection unchecked
+					columnReaders[i].readToVector(rowsRead, batch.writableVectors[i]);
+				}
+			}
+
+			rowsReturned += rowsRead;
+			batch.columnarBatch.setNumRows(rowsRead);
+			return true;
+		}
+
+		private void readNextRowGroup() throws IOException {
+			PageReadStore pages = reader.readNextRowGroup();
+			if (pages == null) {
+				throw new IOException("expecting more rows but reached last block. Read "
+					+ rowsReturned + " out of " + totalRowCount);
+			}
+			List<ColumnDescriptor> columns = requestedSchema.getColumns();
+			columnReaders = new AbstractColumnReader[columns.size()];
+			for (int i = 0; i < columns.size(); ++i) {
+				columnReaders[i] = createColumnReader(
+					isUtcTimestamp,
+					projectedTypes[i],
+					columns.get(i),
+					pages.getPageReader(columns.get(i)));
+			}
+			totalCountLoadedSoFar += pages.getRowCount();
+		}
+
+		public void seek(long rowCount) {
+			if (totalCountLoadedSoFar != 0) {
+				throw new UnsupportedOperationException("Only support seek at first.");
+			}
+
+			List<BlockMetaData> blockMetaData = reader.getRowGroups();
+
+			for (BlockMetaData metaData : blockMetaData) {
+				if (metaData.getRowCount() > rowCount) {
+					break;
+				} else {
+					reader.skipNextRowGroup();
+
+					// skip row group in native reader too
+					if (nativeReaderWrapper != null) {
+						nativeReaderWrapper.skipNextRowGroup();
+					}
+
+					rowsReturned += metaData.getRowCount();
+					totalCountLoadedSoFar += metaData.getRowCount();
+					rowCount -= metaData.getRowCount();
+				}
+			}
+
+			this.recordsToSkip = rowCount;
+		}
+
+		private ParquetReaderBatch<T> getCachedEntry() throws IOException {
+			try {
+				return pool.pollEntry();
+			} catch (InterruptedException e) {
+				Thread.currentThread().interrupt();
+				throw new IOException("Interrupted");
+			}
+		}
+
+		private void skipRecord(RecordIterator<T> records) {
+			while (recordsToSkip > 0 && records.next() != null) {
+				recordsToSkip--;
+			}
+		}
+
+		@Override
+		public void close() throws IOException {
+			if (reader != null) {
+				reader.close();
+				reader = null;
+			}
+
+			if (nativeReaderWrapper != null) {
+				nativeReaderWrapper.close();
+				nativeReaderWrapper = null;
+			}
+		}
+	}
+
+	// ----------------------- Abstract method and class --------------------------
+
+	/**
+	 * @param writableVectors vectors to be write
+	 * @param columnarBatch vectors to be read
+	 * @param recycler batch recycler
+	 */
+	protected abstract ParquetReaderBatch<T> createReaderBatch(
+		WritableColumnVector[] writableVectors,
+		VectorizedColumnBatch columnarBatch, Pool.Recycler<ParquetReaderBatch<T>> recycler);
+
+	/**
+	 * Reader batch that provides writing and reading capabilities. Provides
+	 * {@link RecordIterator} reading interface from
+	 * {@link #convertAndGetIterator(long)}.
+	 */
+	protected abstract static class ParquetReaderBatch<T> {
+
+		private final WritableColumnVector[] writableVectors;
+		protected final VectorizedColumnBatch columnarBatch;
+		private final Pool.Recycler<ParquetReaderBatch<T>> recycler;
+
+		protected ParquetReaderBatch(
+			WritableColumnVector[] writableVectors, VectorizedColumnBatch columnarBatch,
+			Pool.Recycler<ParquetReaderBatch<T>> recycler) {
+			this.writableVectors = writableVectors;
+			this.columnarBatch = columnarBatch;
+			this.recycler = recycler;
+		}
+
+		public void recycle() {
+			recycler.recycle(this);
+		}
+
+		/**
+		 * Provides reading iterator after the records are written to the
+		 * {@link #columnarBatch}.
+		 *
+		 * @param rowsReturned The number of rows that have been returned before this
+		 * 	batch.
+		 */
+		public abstract RecordIterator<T> convertAndGetIterator(long rowsReturned) throws IOException;
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/source/abilities/SupportsAggregationPushDown.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/source/abilities/SupportsAggregationPushDown.java
new file mode 100644
index 0000000000..08a9a26235
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/connector/source/abilities/SupportsAggregationPushDown.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.connector.source.abilities;
+
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.utils.ape.AggregateExprs;
+
+import java.util.List;
+
+/**
+ * For particular agg functions (eg. SUM, MAX, MIN, AVG, COUNT), the workload can be pushed down
+ * to data source.
+ * Commonly, filters should be pushed down too so that the data source can get the partial
+ * aggregation result.
+ *
+ * <p>
+ * For example,
+ * 	select sum(d_date_sk) from date_dim where d_year = 2000;
+ * The original plan of above sql in Flink may be:
+ * == Optimized Logical Plan ==
+ * HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS EXPR$0])
+ * +- Exchange(distribution=[single])
+ *    +- LocalHashAggregate(select=[Partial_SUM(d_date_sk) AS sum$0])
+ *       +- Calc(select=[d_date_sk], where=[=(d_year, 2000)])
+ *          +- TableSourceScan(table=[[myhive, tpcds_1g_snappy, date_dim, filter=[], project=[d_date_sk, d_year]]], fields=[d_date_sk, d_year])
+ *
+ * The plan after filters are pushed down:
+ *== Optimized Logical Plan ==
+ * HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS EXPR$0])
+ * +- Exchange(distribution=[single])
+ *    +- LocalHashAggregate(select=[Partial_SUM(d_date_sk) AS sum$0])
+ *       +- TableSourceScan(table=[[myhive, tpcds_1g_snappy, date_dim, filter=[equals(d_year, 2000)], project=[d_date_sk]]], fields=[d_date_sk])
+ *</p>
+ *
+ * <p>
+ * Moreover, aggregations are pushed down, the optimized plan becomes:
+ * == Optimized Logical Plan ==
+ * HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS EXPR$0])
+ * +- Exchange(distribution=[single])
+ *    +- TableSourceScan(table=[[myhive, tpcds_1g_snappy, date_dim, filter=[equals(d_year, 2000)], project=[d_date_sk], aggregation=[sum$0]]], fields=[sum$0])
+ *</p>
+ */
+public interface SupportsAggregationPushDown {
+
+	boolean applyAggregations(
+		List<String> outputNames,
+		List<DataType> outputTypes,
+		AggregateExprs aggregateExprs);
+
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExpr.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExpr.java
new file mode 100644
index 0000000000..d1cf0e609c
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExpr.java
@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.utils.ape;
+
+import java.io.Serializable;
+import java.util.Arrays;
+
+/**
+ * Aggregate expression super class.
+ */
+public class AggregateExpr implements Serializable {
+	protected String exprName;
+	protected String dataType;
+
+	// optional attributes
+	protected String aliasName;
+	protected String castType;
+	protected Boolean promotePrecision;
+	protected Boolean checkOverflow;
+	protected String checkOverflowType;
+	protected Boolean nullOnOverFlow;
+
+	public AggregateExpr(String exprName, String dataType) {
+		this.exprName = exprName;
+		this.dataType = dataType;
+	}
+
+	public String getExprName() {
+		return exprName;
+	}
+
+	public String getDataType() {
+		return dataType;
+	}
+
+	public void setDataType(String dataType) {
+		this.dataType = dataType;
+	}
+
+	public void setExprName(String exprName) {
+		this.exprName = exprName;
+	}
+
+	public String getAliasName() {
+		return aliasName;
+	}
+
+	public void setAliasName(String aliasName) {
+		this.aliasName = aliasName;
+	}
+
+	public String getCastType() {
+		return castType;
+	}
+
+	public void setCastType(String castType) {
+		this.castType = castType;
+	}
+
+	public Boolean getPromotePrecision() {
+		return promotePrecision;
+	}
+
+	public void setPromotePrecision(Boolean promotePrecision) {
+		this.promotePrecision = promotePrecision;
+	}
+
+	public Boolean getCheckOverflow() {
+		return checkOverflow;
+	}
+
+	public void setCheckOverflow(Boolean checkOverflow) {
+		this.checkOverflow = checkOverflow;
+	}
+
+	public String getCheckOverflowType() {
+		return checkOverflowType;
+	}
+
+	public void setCheckOverflowType(String checkOverflowType) {
+		this.checkOverflowType = checkOverflowType;
+	}
+
+	public Boolean getNullOnOverFlow() {
+		return nullOnOverFlow;
+	}
+
+	public void setNullOnOverFlow(Boolean nullOnOverFlow) {
+		this.nullOnOverFlow = nullOnOverFlow;
+	}
+
+	public void translateDataTypeName() {
+		dataType = translateDataTypeNameInternal(dataType);
+		checkOverflowType = translateDataTypeNameInternal(checkOverflowType);
+	}
+
+	private String translateDataTypeNameInternal(String dataType) {
+		if (dataType != null) {
+			if (Arrays.asList("BooleanType", "IntegerType", "LongType", "FloatType", "DoubleType",
+					"StringType").contains(dataType)
+				|| dataType.startsWith("DecimalType")) {
+				// no need to translate
+				return dataType;
+			}
+
+			if (Arrays.asList("BOOLEAN", "INTEGER", "FLOAT", "DOUBLE")
+				.contains(dataType)) {
+				dataType = dataType.substring(0, 1).toUpperCase()
+					+ dataType.substring(1).toLowerCase()
+					+ "Type";
+			} else if ("BIGINT".equals(dataType)) {
+				dataType = "LongType";
+			} else if (dataType.startsWith("VARCHAR")) {
+				dataType = "StringType";
+			} else if (dataType.startsWith("DECIMAL")) {
+				dataType = dataType.replace("DECIMAL", "DecimalType");
+			} else {
+				throw new RuntimeException("Unsupported data type name in AggregateExpr");
+			}
+		}
+
+		return dataType;
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprBinaryOper.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprBinaryOper.java
new file mode 100644
index 0000000000..2b95384d83
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprBinaryOper.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.utils.ape;
+
+/**
+ * Aggregate expression binary operator.
+ */
+public class AggregateExprBinaryOper extends AggregateExpr {
+
+	private AggregateExpr leftNode;
+	private AggregateExpr rightNode;
+
+	public AggregateExprBinaryOper(
+		Type type,
+		String dataType,
+		AggregateExpr leftNode,
+		AggregateExpr rightNode) {
+		super(type.name, dataType);
+		this.dataType = dataType;
+		this.leftNode = leftNode;
+		this.rightNode = rightNode;
+	}
+
+	public AggregateExpr getLeftNode() {
+		return leftNode;
+	}
+
+	public void setLeftNode(AggregateExpr leftNode) {
+		this.leftNode = leftNode;
+	}
+
+	public AggregateExpr getRightNode() {
+		return rightNode;
+	}
+
+	public void setRightNode(AggregateExpr rightNode) {
+		this.rightNode = rightNode;
+	}
+
+	/**
+	 * Enum for supported binary operations.
+	 */
+	public enum Type {
+		ADD("Add"),
+		SUBTRACT("Subtract"),
+		MULTIPLY("Multiply"),
+		DIVIDE("Divide"),
+		MOD("Mod");
+
+		public String name;
+		Type(String n) {
+			name = n;
+		}
+
+		public static Type fromSqlKind(String sql) {
+			switch (sql) {
+				case "PLUS":
+					return Type.ADD;
+				case "MINUS":
+					return Type.SUBTRACT;
+				case "TIMES":
+					return Type.MULTIPLY;
+				case "DIVIDE":
+					return Type.DIVIDE;
+				case "MOD":
+					return Type.MOD;
+				default:
+					return null;
+			}
+		}
+	}
+
+	@Override
+	public void translateDataTypeName() {
+		super.translateDataTypeName();
+
+		leftNode.translateDataTypeName();
+		rightNode.translateDataTypeName();
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprColumn.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprColumn.java
new file mode 100644
index 0000000000..e4b21af4e3
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprColumn.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.utils.ape;
+
+/**
+ * Aggregate expression column.
+ */
+public class AggregateExprColumn extends AggregateExpr {
+	public static final String DEFAULT_NAME = "AttributeReference";
+
+	private String columnName;
+
+	public AggregateExprColumn(String dataType, String columnName) {
+		super(DEFAULT_NAME, dataType);
+		this.dataType = dataType;
+		this.columnName = columnName;
+	}
+
+	public String getColumnName() {
+		return columnName;
+	}
+
+	public void setColumnName(String columnName) {
+		this.columnName = columnName;
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprLiteral.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprLiteral.java
new file mode 100644
index 0000000000..f8173fc7d3
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprLiteral.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.utils.ape;
+
+/**
+ * Aggregate expression literal.
+ */
+public class AggregateExprLiteral extends AggregateExpr {
+	public static final String DEFAULT_NAME = "Literal";
+
+	private String value;
+
+	public AggregateExprLiteral(String dataType, String value) {
+		super(DEFAULT_NAME, dataType);
+		this.dataType = dataType;
+		this.value = value;
+	}
+
+	public String getValue() {
+		return value;
+	}
+
+	public void setValue(String value) {
+		this.value = value;
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRoot.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRoot.java
new file mode 100644
index 0000000000..68afab3ae5
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRoot.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.utils.ape;
+
+/**
+ * Aggregate expression root.
+ */
+public class AggregateExprRoot extends AggregateExpr {
+	public static final String DEFAULT_NAME = "RootAgg";
+
+	private boolean isDistinct;
+
+	private AggregateExprRootChild child;
+
+	public AggregateExprRoot(
+		boolean isDistinct,
+		AggregateExprRootChild child) {
+		super(DEFAULT_NAME, null);
+		this.isDistinct = isDistinct;
+		this.child = child;
+	}
+
+	public boolean getIsDistinct() {
+		return isDistinct;
+	}
+
+	public void setIsDistinct(boolean distinct) {
+		isDistinct = distinct;
+	}
+
+	public AggregateExpr getChild() {
+		return child;
+	}
+
+	public void setChild(AggregateExprRootChild child) {
+		this.child = child;
+	}
+
+	@Override
+	public void translateDataTypeName() {
+		super.translateDataTypeName();
+
+		child.translateDataTypeName();
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRootChild.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRootChild.java
new file mode 100644
index 0000000000..47427fd5b9
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprRootChild.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.utils.ape;
+
+/**
+ * Aggregate expression root child.
+ */
+public class AggregateExprRootChild extends AggregateExpr {
+
+	private AggregateExpr child;
+
+	public AggregateExprRootChild(Type type, AggregateExpr c) {
+		super(type.name, null);
+		child = c;
+	}
+
+	public AggregateExpr getChild() {
+		return child;
+	}
+
+	public void setChild(AggregateExpr child) {
+		this.child = child;
+	}
+
+	/**
+	 * Enum of supported aggregate operations.
+	 */
+	public enum Type {
+		MAX("Max"),
+		MIN("Min"),
+		COUNT("Count"),
+		SUM("Sum"); /* `Average` will be parse to `Sum` and `Count`. So there is no `Average` */
+
+		public String name;
+		Type(String n) {
+			name = n;
+		}
+
+		public static Type fromSqlKind(String sql) {
+			switch (sql) {
+				case "max":
+					return Type.MAX;
+				case "min":
+					return Type.MIN;
+				case "count":
+				case "count1":
+					return Type.COUNT;
+				case "sum":
+					return Type.SUM;
+				default:
+					return null;
+			}
+		}
+	}
+
+	@Override
+	public void translateDataTypeName() {
+		super.translateDataTypeName();
+
+		child.translateDataTypeName();
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprs.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprs.java
new file mode 100644
index 0000000000..39cb664691
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/ape/AggregateExprs.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.utils.ape;
+
+import java.io.Serializable;
+import java.util.List;
+
+/**
+ * Class to hold the whole aggregate expressions to be pushed down.
+ */
+public class AggregateExprs implements Serializable {
+	private List<AggregateExpr> groupByExprs;
+	private List<AggregateExpr> aggregateExprs;
+
+	public AggregateExprs(List<AggregateExpr> groupByExprs, List<AggregateExpr> aggregateExprs) {
+		this.groupByExprs = groupByExprs;
+		this.aggregateExprs = aggregateExprs;
+	}
+
+	public List<AggregateExpr> getGroupByExprs() {
+		return groupByExprs;
+	}
+
+	public void setGroupByExprs(List<AggregateExpr> groupByExprs) {
+		this.groupByExprs = groupByExprs;
+	}
+
+	public List<AggregateExpr> getAggregateExprs() {
+		return aggregateExprs;
+	}
+
+	public void setAggregateExprs(List<AggregateExpr> aggregateExprs) {
+		this.aggregateExprs = aggregateExprs;
+	}
+
+	public void translateDataTypeName() {
+		aggregateExprs.forEach(AggregateExpr::translateDataTypeName);
+	}
+}
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala
index 32d717e13a..92e6bda71c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/FlinkBatchRuleSets.scala
@@ -443,6 +443,8 @@ object FlinkBatchRuleSets {
     */
   val PHYSICAL_REWRITE: RuleSet = RuleSets.ofList(
     EnforceLocalHashAggRule.INSTANCE,
-    EnforceLocalSortAggRule.INSTANCE
+    EnforceLocalSortAggRule.INSTANCE,
+    PushLocalHashAggIntoTableSourceScanRule.INSTANCE,
+    PushLocalHashAggCalcIntoTableSourceScanRule.INSTANCE
   )
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchExecAggRuleBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchExecAggRuleBase.scala
index 49747b4525..0e89d279c5 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchExecAggRuleBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/BatchExecAggRuleBase.scala
@@ -29,17 +29,19 @@ import org.apache.flink.table.planner.plan.utils.{AggregateUtil, FlinkRelOptUtil
 import org.apache.flink.table.planner.utils.AggregatePhaseStrategy
 import org.apache.flink.table.planner.utils.TableConfigUtils.getAggPhaseStrategy
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
-import org.apache.flink.table.types.DataType
-import org.apache.flink.table.types.logical.LogicalType
-
+import org.apache.flink.table.types.{AtomicDataType, DataType}
+import org.apache.flink.table.types.logical.{IntType, LogicalType}
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.core.{Aggregate, AggregateCall}
 import org.apache.calcite.rel.{RelCollation, RelCollations, RelFieldCollation, RelNode}
+import org.apache.calcite.sql.`type`.SqlTypeName
 import org.apache.calcite.tools.RelBuilder
 import org.apache.calcite.util.Util
+import org.apache.flink.table.utils.ape.{AggregateExpr, AggregateExprLiteral, AggregateExprRoot, AggregateExprRootChild}
 
 import scala.collection.JavaConversions._
+import scala.collection.mutable.ArrayBuffer
 
 trait BatchExecAggRuleBase {
 
@@ -235,4 +237,63 @@ trait BatchExecAggRuleBase {
     }
   }
 
+  def flattenAggFunctions(localAgg: BatchExecLocalHashAggregate,
+                          aggOutputTypes: Array[Array[DataType]],
+                          inputColumns: ArrayBuffer[AggregateExpr])
+  : Option[ArrayBuffer[AggregateExprRoot]] = {
+
+    val aggregateColumns = new ArrayBuffer[AggregateExprRoot]
+    var aggIndex = 0
+    val typeFactory = localAgg.getCluster.getTypeFactory.asInstanceOf[FlinkTypeFactory]
+    aggOutputTypes.foreach(types => {
+      val aggCall = localAgg.getAggCallList.get(aggIndex)
+      val aggCallFunction = localAgg.getAggCallToAggFunction.map(_._2).get(aggIndex)
+      val isDistinct = aggCall.isDistinct
+
+      // get agg function names
+      val aggFuncNames = aggCallFunction match {
+        case aggFunc: DeclarativeAggregateFunction =>
+          aggFunc.aggBufferAttributes().map(_.getName)
+        case _ => new Array[String](0)
+      }
+      if (aggFuncNames.length == 0 || aggFuncNames.length != types.length) {
+        return None
+      }
+
+      // in this rule, arg list should mostly contains 1 argument. count1 have 0 argument
+      if (aggCall.getArgList.length > 1
+          || (aggCall.getArgList.isEmpty && !aggFuncNames(0).equals("count1"))) {
+        return None
+      }
+
+      // flatten output types of each agg call
+      val column = if(aggCall.getArgList.nonEmpty) {
+        inputColumns.get(aggCall.getArgList.get(0))
+      } else {
+        val dataType = typeFactory.createSqlType(SqlTypeName.INTEGER)
+        new AggregateExprLiteral(dataType.toString, "1")
+      }
+      var typeIndex = 0
+      types.foreach(_ => {
+        val aggType = AggregateExprRootChild.Type.fromSqlKind(aggFuncNames(typeIndex))
+
+        // check if not supported
+        if (aggType == null) {
+          return None
+        }
+
+        val aggChild = new AggregateExprRootChild(aggType, column)
+        val aggRoot = new AggregateExprRoot(isDistinct, aggChild)
+        aggregateColumns.add(aggRoot)
+
+        typeIndex += 1
+      })
+
+      aggIndex += 1
+    })
+
+    Some(aggregateColumns)
+  }
+
+
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggCalcIntoTableSourceScanRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggCalcIntoTableSourceScanRule.scala
new file mode 100644
index 0000000000..c6e3ae2df8
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggCalcIntoTableSourceScanRule.scala
@@ -0,0 +1,233 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.table.planner.plan.rules.physical.batch
+
+import org.apache.calcite.plan.RelOptRule.{any, operand}
+import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall}
+import org.apache.calcite.rex.{RexCall, RexInputRef, RexLiteral, RexLocalRef}
+import org.apache.calcite.sql.SqlBinaryOperator
+import org.apache.flink.table.connector.source.abilities.SupportsAggregationPushDown
+import org.apache.flink.table.planner.calcite.FlinkTypeFactory
+import org.apache.flink.table.planner.plan.nodes.FlinkConventions
+import org.apache.flink.table.planner.plan.nodes.physical.batch.{BatchExecCalc, BatchExecExchange, BatchExecLocalHashAggregate, BatchExecTableSourceScan}
+import org.apache.flink.table.planner.plan.schema.TableSourceTable
+import org.apache.flink.table.planner.plan.utils.AggregateUtil
+import org.apache.flink.table.types.utils.TypeConversions
+import org.apache.flink.table.utils.ape._
+
+import scala.collection.JavaConversions._
+import scala.collection.mutable.ArrayBuffer
+
+/**
+ * Some aggregation functions are carried out tow-phase aggregate with calc, that is:
+ * BatchExecTableSourceScan
+ *   -> BatchExecCalc
+ *     -> BatchExecLocalHashAggregate
+ *       -> BatchExecExchange
+ *         -> BatchExecHashAggregate
+ *
+ * If the `input` mentioned above can processing agg functions partially, then we can push down
+ * aggregations and calculation to the input.
+ * After that `LocalHashAggregate` and `Calc` should be removed from the plan.
+ */
+class PushLocalHashAggCalcIntoTableSourceScanRule extends RelOptRule(
+  operand(classOf[BatchExecExchange],
+    operand(classOf[BatchExecLocalHashAggregate],
+      operand(classOf[BatchExecCalc],
+        operand(classOf[BatchExecTableSourceScan], FlinkConventions.BATCH_PHYSICAL, any)))),
+  "PushLocalHashAggIntoTableSourceScanRule")
+  with BatchExecAggRuleBase {
+
+  override def onMatch(call: RelOptRuleCall): Unit = {
+    val exchange = call.rels(0).asInstanceOf[BatchExecExchange]
+    val localAgg = call.rels(1).asInstanceOf[BatchExecLocalHashAggregate]
+    val calc = call.rels(2).asInstanceOf[BatchExecCalc]
+    val input = call.rels(3).asInstanceOf[BatchExecTableSourceScan]
+
+    val tableSourceTable = input.getTable.unwrap(classOf[TableSourceTable])
+
+    // check if source table applicable
+    if (tableSourceTable != null
+        && tableSourceTable.tableSource.isInstanceOf[SupportsAggregationPushDown]
+        && !tableSourceTable.extraDigests.exists(str => str.startsWith("aggregation=["))) {
+
+      // apply aggregations to table source
+      val newTableSource = tableSourceTable.tableSource.copy
+
+      // make agg exprs
+      val aggregateExprs = makeAggregateExprs(localAgg, calc, input)
+
+      // apply agg to table source
+      var applied = false
+      if (aggregateExprs.isDefined) {
+        applied =
+            newTableSource.asInstanceOf[SupportsAggregationPushDown].applyAggregations(
+              localAgg.getRowType.getFieldList.map(_.getName),
+              localAgg.getRowType.getFieldList.map(
+                f =>
+                  TypeConversions.fromLogicalToDataType(FlinkTypeFactory.toLogicalType(f.getType))
+              ),
+              aggregateExprs.get
+            )
+      }
+
+      // replace local agg with new scan
+      if (applied) {
+        val newTableSourceTable = tableSourceTable.copy(
+          newTableSource,
+          localAgg.getRowType,
+          Array[String]("aggregation=[" + localAgg.getRowType.getFieldNames.mkString(",") + "]"))
+
+        val newScan = new BatchExecTableSourceScan(
+          input.getCluster, input.getTraitSet, newTableSourceTable)
+
+        // replace input of exchange
+        val newExchange = exchange.copy(exchange.getTraitSet, Seq(newScan))
+        call.transformTo(newExchange)
+      }
+    }
+  }
+
+  def makeAggregateExprs(localAgg: BatchExecLocalHashAggregate,
+                         calc: BatchExecCalc,
+                         input: BatchExecTableSourceScan
+                        ): Option[AggregateExprs] = {
+
+    // 1. wrap input columns of table with expression class
+    val inputColumns = new ArrayBuffer[AggregateExprColumn]
+    input.getRowType.getFieldList.foreach(field => {
+      inputColumns.add(new AggregateExprColumn(field.getType.toString, field.getName))
+    })
+
+    // 2. collect calc columns from input columns
+    val calcOutputColumns = getAggregateExprsFromCalc(calc, inputColumns)
+    if (calcOutputColumns.isEmpty) {
+      return None
+    }
+
+    // 3. collect grouped columns from calc columns
+    val groups = new ArrayBuffer[AggregateExpr]
+    localAgg.getGrouping.foreach(index => {
+      calcOutputColumns.get(index) match {
+        case c: AggregateExprColumn => groups.add(c)
+        case _ => return None
+      }
+    })
+
+    // 4. collect output columns of agg functions from input columns
+    // some functions may produce more than 1 type. eg. sum
+
+    // parse output types of agg functions
+    val (_, aggOutputTypes, _) = AggregateUtil.transformToBatchAggregateFunctions(
+      localAgg.getAggCallList, localAgg.getInput(0).getRowType)
+
+    val aggregateColumns = flattenAggFunctions(localAgg, aggOutputTypes, calcOutputColumns.get)
+    if (aggregateColumns.isEmpty) {
+      return None
+    }
+
+    // 5. wrap all output columns into 1 expression object
+    val aggregates = new ArrayBuffer[AggregateExpr]
+    val calcColumnNames = calc.getRowType.getFieldNames
+    var aggColumnIndex = 0
+
+    localAgg.getRowType.getFieldList.foreach(field => {
+      val calcColumnIndex = calcColumnNames.indexOf(field.getName)
+      if (calcColumnIndex >= 0) {
+        // this output column is from input
+        aggregates.add(calcOutputColumns.get(calcColumnIndex))
+      } else {
+        // this output column is from agg functions
+        val aggRoot = aggregateColumns.get(aggColumnIndex)
+        aggRoot.setDataType(field.getType.toString)
+        aggRoot.getChild.setDataType(field.getType.toString)
+        aggRoot.setAliasName(field.getName)
+        aggregates.add(aggRoot)
+
+        aggColumnIndex += 1
+      }
+    })
+
+    val aggregateExprs = new AggregateExprs(groups, aggregates)
+    Some(aggregateExprs)
+  }
+
+  def getAggregateExprsFromCalc(calc: BatchExecCalc, inputColumns: ArrayBuffer[AggregateExprColumn])
+  : Option[ArrayBuffer[AggregateExpr]] = {
+
+    val program = calc.getProgram
+
+    // check if condition exists. all filters should have been pushed down
+    if (program.getCondition != null) {
+      return None
+    }
+
+    // make temp expr columns
+    val calcExprColumns = new ArrayBuffer[AggregateExpr]
+    program.getExprList.foreach {
+      case input: RexInputRef =>
+        calcExprColumns.add(inputColumns.get(input.getIndex))
+
+      case literal: RexLiteral =>
+        calcExprColumns.add(
+          new AggregateExprLiteral(literal.getType.toString, literal.getValue.toString)
+        )
+
+      case call: RexCall =>
+        call.getOperator match {
+          case binaryOp: SqlBinaryOperator =>
+            val opType = AggregateExprBinaryOper.Type.fromSqlKind(binaryOp.kind.sql)
+            if (opType != null
+                && call.getOperands.get(0).isInstanceOf[RexLocalRef]
+                && call.getOperands.get(1).isInstanceOf[RexLocalRef]) {
+              val index0 = call.getOperands.get(0).asInstanceOf[RexLocalRef].getIndex
+              val index1 = call.getOperands.get(1).asInstanceOf[RexLocalRef].getIndex
+              val binaryExpr = new AggregateExprBinaryOper(
+                opType,
+                call.getType.toString,
+                calcExprColumns.get(index0),
+                calcExprColumns.get(index1)
+              )
+              if (call.getType != null) {
+                binaryExpr.setCheckOverflow(true)
+                binaryExpr.setCheckOverflowType(call.getType.toString)
+              }
+              calcExprColumns.add(binaryExpr)
+            } else {
+              return None
+            }
+          case _ =>
+            return None
+        }
+
+      case _ =>
+        return None
+    }
+
+    // project columns
+    val calcOutputColumns = new ArrayBuffer[AggregateExpr]
+    program.getProjectList.foreach(r => calcOutputColumns.add(calcExprColumns.get(r.getIndex)))
+
+    Some(calcOutputColumns)
+  }
+
+}
+
+object PushLocalHashAggCalcIntoTableSourceScanRule {
+  val INSTANCE = new PushLocalHashAggCalcIntoTableSourceScanRule()
+}
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggIntoTableSourceScanRule.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggIntoTableSourceScanRule.scala
new file mode 100644
index 0000000000..7cb51ed8b3
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/rules/physical/batch/PushLocalHashAggIntoTableSourceScanRule.scala
@@ -0,0 +1,160 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.table.planner.plan.rules.physical.batch
+
+import org.apache.calcite.plan.RelOptRule.{any, operand}
+import org.apache.calcite.plan.{RelOptRule, RelOptRuleCall}
+import org.apache.flink.table.connector.source.abilities.SupportsAggregationPushDown
+import org.apache.flink.table.planner.calcite.FlinkTypeFactory
+import org.apache.flink.table.planner.plan.nodes.FlinkConventions
+import org.apache.flink.table.planner.plan.nodes.physical.batch.{BatchExecExchange, BatchExecLocalHashAggregate, BatchExecTableSourceScan}
+import org.apache.flink.table.planner.plan.schema.TableSourceTable
+import org.apache.flink.table.planner.plan.utils.AggregateUtil
+import org.apache.flink.table.types.utils.TypeConversions
+import org.apache.flink.table.utils.ape.{AggregateExpr, AggregateExprColumn, AggregateExprs}
+
+import scala.collection.JavaConversions._
+import scala.collection.mutable.ArrayBuffer
+
+/**
+ * Some aggregation functions are carried out tow-phase aggregate, that is:
+ * BatchExecTableSourceScan
+ *   -> BatchExecLocalHashAggregate
+ *     -> BatchExecExchange
+ *       -> BatchExecHashAggregate
+ *
+ * If the `input` mentioned above can processing agg functions partially, then we can push down
+ * aggregations to the input. And the `LocalHashAggregate` should be removed from the plan.
+ */
+class PushLocalHashAggIntoTableSourceScanRule extends RelOptRule(
+  operand(classOf[BatchExecExchange],
+    operand(classOf[BatchExecLocalHashAggregate],
+      operand(classOf[BatchExecTableSourceScan], FlinkConventions.BATCH_PHYSICAL, any))),
+  "PushLocalHashAggIntoTableSourceScanRule")
+  with BatchExecAggRuleBase {
+
+  override def onMatch(call: RelOptRuleCall): Unit = {
+    val exchange = call.rels(0).asInstanceOf[BatchExecExchange]
+    val localAgg = call.rels(1).asInstanceOf[BatchExecLocalHashAggregate]
+    val input = call.rels(2).asInstanceOf[BatchExecTableSourceScan]
+
+    val tableSourceTable = input.getTable.unwrap(classOf[TableSourceTable])
+
+    // check if source table applicable
+    if (tableSourceTable != null
+        && tableSourceTable.tableSource.isInstanceOf[SupportsAggregationPushDown]
+        && !tableSourceTable.extraDigests.exists(str => str.startsWith("aggregation=["))) {
+
+      // apply aggregations to table source
+      val newTableSource = tableSourceTable.tableSource.copy
+
+      // make agg exprs
+      val aggregateExprs = makeAggregateExprs(localAgg, input)
+
+      // apply agg to table source
+      var applied = false
+      if (aggregateExprs.isDefined) {
+        applied =
+          newTableSource.asInstanceOf[SupportsAggregationPushDown].applyAggregations(
+            localAgg.getRowType.getFieldList.map(_.getName),
+            localAgg.getRowType.getFieldList.map(
+              f => TypeConversions.fromLogicalToDataType(FlinkTypeFactory.toLogicalType(f.getType))
+            ),
+            aggregateExprs.get
+          )
+      }
+
+      // replace local agg with new scan
+      if (applied) {
+        val newTableSourceTable = tableSourceTable.copy(
+          newTableSource,
+          localAgg.getRowType,
+          Array[String]("aggregation=[" + localAgg.getRowType.getFieldNames.mkString(",") + "]"))
+
+        val newScan = new BatchExecTableSourceScan(
+          input.getCluster, input.getTraitSet, newTableSourceTable)
+
+        // replace input of exchange
+        val newExchange = exchange.copy(exchange.getTraitSet, Seq(newScan))
+        call.transformTo(newExchange)
+      }
+    }
+  }
+
+  def makeAggregateExprs(localAgg: BatchExecLocalHashAggregate,
+                         input: BatchExecTableSourceScan
+                        ): Option[AggregateExprs] = {
+
+    // 1. wrap input columns of table with expression class
+    val inputColumns = new ArrayBuffer[AggregateExpr]
+    input.getRowType.getFieldList.foreach(field => {
+      inputColumns.add(new AggregateExprColumn(field.getType.toString, field.getName))
+    })
+
+    // 2. collect grouped columns from input columns
+    val groups = new ArrayBuffer[AggregateExpr]
+    localAgg.getGrouping.foreach(index => {
+      inputColumns.get(index) match {
+        case c: AggregateExprColumn => groups.add(c)
+        case _ => return None
+      }
+    })
+
+    // 3. collect output columns of agg functions from input columns
+    // some functions may produce more than 1 type. eg. sum
+
+    // parse output types of agg functions
+    val (_, aggOutputTypes, _) = AggregateUtil.transformToBatchAggregateFunctions(
+      localAgg.getAggCallList, localAgg.getInput(0).getRowType)
+
+    val aggregateColumns = flattenAggFunctions(localAgg, aggOutputTypes, inputColumns)
+    if (aggregateColumns.isEmpty) {
+      return None
+    }
+
+    // 4. wrap all output columns into 1 expression object
+    val aggregates = new ArrayBuffer[AggregateExpr]
+    val inputColumnNames = input.getRowType.getFieldNames
+    var aggColumnIndex = 0
+
+    localAgg.getRowType.getFieldList.foreach(field => {
+      val inputColumnIndex = inputColumnNames.indexOf(field.getName)
+      if (inputColumnIndex >= 0) {
+        // this output column is from input
+        aggregates.add(inputColumns.get(inputColumnIndex))
+      } else {
+        // this output column is from agg functions
+        val aggRoot = aggregateColumns.get(aggColumnIndex)
+        aggRoot.setDataType(field.getType.toString)
+        aggRoot.getChild.setDataType(field.getType.toString)
+        aggRoot.setAliasName(field.getName)
+        aggregates.add(aggRoot)
+
+        aggColumnIndex += 1
+      }
+    })
+
+    val aggregateExprs = new AggregateExprs(groups, aggregates)
+    Some(aggregateExprs)
+  }
+
+}
+
+object PushLocalHashAggIntoTableSourceScanRule {
+  val INSTANCE = new PushLocalHashAggIntoTableSourceScanRule
+}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java
index b4d6782866..9454eb219b 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/LimitableBulkFormat.java
@@ -123,8 +123,14 @@ public class LimitableBulkFormat<T, SplitT extends FileSourceSplit> implements B
 				if (reachLimit()) {
 					return null;
 				}
-				numRead.incrementAndGet();
-				return iterator.next();
+
+				RecordAndPosition<T> ret = iterator.next();
+
+				if (ret != null) {
+					numRead.incrementAndGet();
+				}
+
+				return ret;
 			}
 
 			@Override
-- 
2.28.0.windows.1

